INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/GM2017-july-sep-GRID.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.4111352264881134
INFO:__main__:Meta Train Loss: 0.41340672969818115
INFO:__main__:Meta Train Loss: 0.41025635600090027
INFO:__main__:Meta Train Loss: 0.4103878140449524
INFO:__main__:Meta Train Loss: 0.41005367040634155
INFO:__main__:Meta Train Loss: 0.4093984365463257
INFO:__main__:Meta Train Loss: 0.40827468037605286
INFO:__main__:Meta Train Loss: 0.41198888421058655
INFO:__main__:Meta Train Loss: 0.41620904207229614
INFO:__main__:Meta Train Loss: 0.4128956198692322
INFO:__main__:Meta Train Loss: 0.4147462248802185
INFO:__main__:Meta Train Loss: 0.40940865874290466
INFO:__main__:Meta Train Loss: 0.4122450649738312
INFO:__main__:Meta Train Loss: 0.40980812907218933
INFO:__main__:Meta Train Loss: 0.4061717092990875
INFO:__main__:Meta Train Loss: 0.4085412919521332
INFO:__main__:Meta Train Loss: 0.39881473779678345
INFO:__main__:Meta Train Loss: 0.40629249811172485
INFO:__main__:Meta Train Loss: 0.41268083453178406
INFO:__main__:Meta Train Loss: 0.40885642170906067
INFO:__main__:Meta Train Loss: 0.411161869764328
INFO:__main__:Meta Train Loss: 0.4181750416755676
INFO:__main__:Meta Train Loss: 0.40929126739501953
INFO:__main__:Meta Train Loss: 0.40656331181526184
INFO:__main__:Meta Train Loss: 0.4100876748561859
INFO:__main__:Meta Train Loss: 0.4136137366294861
INFO:__main__:Meta Train Loss: 0.4082789123058319
INFO:__main__:Meta Train Loss: 0.41115856170654297
INFO:__main__:Meta Train Loss: 0.41415441036224365
INFO:__main__:Meta Train Loss: 0.4117123484611511
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.39968377351760864
INFO:__main__:Meta Train Loss: 0.3983292877674103
INFO:__main__:Meta Train Loss: 0.4124436676502228
INFO:__main__:Meta Train Loss: 0.4061157703399658
INFO:__main__:Meta Train Loss: 0.40773889422416687
INFO:__main__:Meta Train Loss: 0.4162787199020386
INFO:__main__:Meta Train Loss: 0.4132029414176941
INFO:__main__:Meta Train Loss: 0.4100002944469452
INFO:__main__:Meta Train Loss: 0.41952696442604065
INFO:__main__:Meta Train Loss: 0.40384775400161743
INFO:__main__:Meta Train Loss: 0.3984558880329132
INFO:__main__:Meta Train Loss: 0.39585432410240173
INFO:__main__:Meta Train Loss: 0.3966785669326782
INFO:__main__:Meta Train Loss: 0.39706042408943176
INFO:__main__:Meta Train Loss: 0.41478732228279114
INFO:__main__:Meta Train Loss: 0.3078976571559906
INFO:__main__:Meta Train Loss: 0.29994893074035645
INFO:__main__:Meta Train Loss: 0.3557693660259247
INFO:__main__:Meta Train Loss: 0.4046688377857208
INFO:__main__:Meta Train Loss: 0.40899449586868286
INFO:__main__:Meta Train Loss: 0.4273666739463806
INFO:__main__:Meta Train Loss: 0.40920546650886536
INFO:__main__:Meta Train Loss: 0.3947320282459259
INFO:__main__:Meta Train Loss: 0.4297609329223633
INFO:__main__:Meta Train Loss: 0.40821075439453125
INFO:__main__:Meta Train Loss: 0.42658767104148865
INFO:__main__:Meta Train Loss: 0.4265601336956024
INFO:__main__:Meta Train Loss: 0.40193119645118713
INFO:__main__:Meta Train Loss: 0.41596922278404236
INFO:__main__:Meta Train Loss: 0.3871868848800659
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.3785255253314972
INFO:__main__:Meta Train Loss: 0.4070587158203125
INFO:__main__:Meta Train Loss: 0.38344836235046387
INFO:__main__:Meta Train Loss: 0.4031781256198883
INFO:__main__:Meta Train Loss: 0.3743785321712494
INFO:__main__:Meta Train Loss: 0.4301597476005554
INFO:__main__:Meta Train Loss: 0.43022313714027405
INFO:__main__:Meta Train Loss: 0.37682661414146423
INFO:__main__:Meta Train Loss: 0.4312755763530731
INFO:__main__:Meta Train Loss: 0.37488478422164917
INFO:__main__:Meta Train Loss: 0.3842150568962097
INFO:__main__:Meta Train Loss: 0.40768495202064514
INFO:__main__:Meta Train Loss: 0.38865917921066284
INFO:__main__:Meta Train Loss: 0.40300440788269043
INFO:__main__:Meta Train Loss: 0.4090563654899597
INFO:__main__:Meta Train Loss: 0.42449232935905457
INFO:__main__:Meta Train Loss: 0.35087913274765015
INFO:__main__:Meta Train Loss: 0.36911001801490784
INFO:__main__:Meta Train Loss: 0.409802109003067
INFO:__main__:Meta Train Loss: 0.4810105860233307
INFO:__main__:Meta Train Loss: 0.44258302450180054
INFO:__main__:Meta Train Loss: 0.39577165246009827
INFO:__main__:Meta Train Loss: 0.3630579710006714
INFO:__main__:Meta Train Loss: 0.46541738510131836
INFO:__main__:Meta Train Loss: 0.4071080982685089
INFO:__main__:Meta Train Loss: 0.40158265829086304
INFO:__main__:Meta Train Loss: 0.4627998173236847
INFO:__main__:Meta Train Loss: 0.3983977437019348
INFO:__main__:Meta Train Loss: 0.4412051737308502
INFO:__main__:Meta Train Loss: 0.3947509527206421
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/yellow-taxi2020-nov-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8597808480262756
INFO:__main__:Meta Train Loss: 0.8601611852645874
INFO:__main__:Meta Train Loss: 0.8608464598655701
INFO:__main__:Meta Train Loss: 0.8603500127792358
INFO:__main__:Meta Train Loss: 0.860205352306366
INFO:__main__:Meta Train Loss: 0.8607927560806274
INFO:__main__:Meta Train Loss: 0.8610062599182129
INFO:__main__:Meta Train Loss: 0.8617350459098816
INFO:__main__:Meta Train Loss: 0.8603389859199524
INFO:__main__:Meta Train Loss: 0.8611969351768494
INFO:__main__:Meta Train Loss: 0.8613664507865906
INFO:__main__:Meta Train Loss: 0.861083984375
INFO:__main__:Meta Train Loss: 0.8607388734817505
INFO:__main__:Meta Train Loss: 0.8604154586791992
INFO:__main__:Meta Train Loss: 0.8598842620849609
INFO:__main__:Meta Train Loss: 0.8603997826576233
INFO:__main__:Meta Train Loss: 0.8615532517433167
INFO:__main__:Meta Train Loss: 0.8607069849967957
INFO:__main__:Meta Train Loss: 0.8595564961433411
INFO:__main__:Meta Train Loss: 0.8601029515266418
INFO:__main__:Meta Train Loss: 0.8608411550521851
INFO:__main__:Meta Train Loss: 0.8602514863014221
INFO:__main__:Meta Train Loss: 0.8607062101364136
INFO:__main__:Meta Train Loss: 0.8602514266967773
INFO:__main__:Meta Train Loss: 0.8607439994812012
INFO:__main__:Meta Train Loss: 0.8613483309745789
INFO:__main__:Meta Train Loss: 0.8604841232299805
INFO:__main__:Meta Train Loss: 0.8601652383804321
INFO:__main__:Meta Train Loss: 0.860632061958313
INFO:__main__:Meta Train Loss: 0.8609489798545837
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8405269980430603
Traceback (most recent call last):
  File "/zhome/2b/7/117471/Thesis/src/models/compare_metalearning.py", line 331, in <module>
    losses = eval_models(
  File "/zhome/2b/7/117471/Thesis/src/models/compare_metalearning.py", line 230, in eval_models
    out = untrained_edgeconv(support_data)
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/zhome/2b/7/117471/Thesis/src/models/models.py", line 567, in forward
    x = self.edgeconv3(x, batch=batched_data.batch)
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/torch_geometric/nn/conv/edge_conv.py", line 112, in forward
    edge_index = knn(x[0], x[1], self.k, b[0], b[1],
KeyboardInterrupt
