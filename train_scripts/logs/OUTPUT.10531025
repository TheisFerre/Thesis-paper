Starting:
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Epoch: 1
Meta Train Loss: 0.9156365394592285
########
Epoch: 2
Meta Train Loss: 0.9524648785591125
########
Epoch: 3
Meta Train Loss: 1.0487552881240845
########
Epoch: 4
Meta Train Loss: 0.7984830141067505
########
Epoch: 5
Meta Train Loss: 0.8025941252708435
########
Epoch: 6
Meta Train Loss: 0.8629041910171509
########
Epoch: 7
Meta Train Loss: 0.9010722637176514
########
Epoch: 8
Meta Train Loss: 0.903984010219574
########
Epoch: 9
Meta Train Loss: 0.891149640083313
########
Epoch: 10
Meta Train Loss: 0.9000772833824158
########
Epoch: 11
Meta Train Loss: 1.4579001665115356
########
Epoch: 12
Meta Train Loss: 0.9082551002502441
########
Epoch: 13
Meta Train Loss: 0.8941183686256409
########
Epoch: 14
Meta Train Loss: 0.8290615677833557
########
Epoch: 15
Meta Train Loss: 0.8331524729728699
########
Epoch: 16
Meta Train Loss: 1.4440433979034424
########
Epoch: 17
Meta Train Loss: 0.9984966516494751
########
Epoch: 18
Meta Train Loss: 0.941828727722168
########
Epoch: 19
Meta Train Loss: 0.9678972363471985
########
Epoch: 20
Meta Train Loss: 1.2462732791900635
########
Epoch: 21
Meta Train Loss: 1.0850861072540283
########
Epoch: 22
Meta Train Loss: 0.635474681854248
########
Epoch: 23
Meta Train Loss: 1.0651540756225586
########
Epoch: 24
Meta Train Loss: 0.9865049719810486
########
Epoch: 25
Meta Train Loss: 0.8603668808937073
########
Epoch: 26
Meta Train Loss: 0.9037739634513855
########
Epoch: 27
Meta Train Loss: 0.8096978664398193
########
Epoch: 28
Meta Train Loss: 1.0857950448989868
########
Epoch: 29
Meta Train Loss: 1.346294641494751
########
Epoch: 30
Meta Train Loss: 1.1627291440963745
########
Epoch: 31
Meta Train Loss: 0.8858813643455505
########
Epoch: 32
Meta Train Loss: 0.9831907153129578
########
Epoch: 33
Meta Train Loss: 0.7434723377227783
########
Epoch: 34
Meta Train Loss: 0.7757179737091064
########
Epoch: 35
Meta Train Loss: 0.7342550158500671
########
Epoch: 36
Meta Train Loss: 1.0722224712371826
########
Epoch: 37
Meta Train Loss: 0.7847442030906677
########
Epoch: 38
Meta Train Loss: 0.5806155204772949
########
Epoch: 39
Meta Train Loss: 0.5834914445877075
########
Epoch: 40
Meta Train Loss: 0.8642933368682861
########
Epoch: 41
Meta Train Loss: 0.6138470768928528
########
Epoch: 42
Meta Train Loss: 0.8255259990692139
########
Epoch: 43
Meta Train Loss: 0.9076955318450928
########
Epoch: 44
Meta Train Loss: 0.8781559467315674
########
Epoch: 45
Meta Train Loss: 0.7205929160118103
########
Epoch: 46
Meta Train Loss: 0.7542432546615601
########
Epoch: 47
Meta Train Loss: 0.8349114060401917
########
Epoch: 48
Meta Train Loss: 0.5632467269897461
########
Epoch: 49
Meta Train Loss: 0.8696622252464294
########
Epoch: 50
Meta Train Loss: 0.4809909164905548
########
Epoch: 51
Meta Train Loss: 0.732880711555481
########
Epoch: 52
Meta Train Loss: 0.9325946569442749
########
Epoch: 53
Meta Train Loss: 0.7843747138977051
########
Epoch: 54
Meta Train Loss: 0.7369328141212463
########
Epoch: 55
Meta Train Loss: 0.6706597805023193
########
Epoch: 56
Meta Train Loss: 1.0167793035507202
########
Epoch: 57
Meta Train Loss: 0.9766099452972412
########
Epoch: 58
Meta Train Loss: 0.9034927487373352
########
Epoch: 59
Meta Train Loss: 0.7336808443069458
########
Epoch: 60
Meta Train Loss: 0.6896820664405823
########
Epoch: 61
Meta Train Loss: 0.8294535279273987
########
Epoch: 62
Meta Train Loss: 0.6978065371513367
########
Epoch: 63
Meta Train Loss: 0.7639985084533691
########
Epoch: 64
Meta Train Loss: 0.6643171906471252
########
Epoch: 65
Meta Train Loss: 0.587335467338562
########
Epoch: 66
Meta Train Loss: 0.7374757528305054
########
Epoch: 67
Meta Train Loss: 0.7100412249565125
########
Epoch: 68
Meta Train Loss: 0.5060468316078186
########
Epoch: 69
Meta Train Loss: 0.7250370979309082
########
Epoch: 70
Meta Train Loss: 0.7511136531829834
########
Epoch: 71
Meta Train Loss: 0.765850305557251
########
Epoch: 72
Meta Train Loss: 0.5619851350784302
########
Epoch: 73
Meta Train Loss: 0.6408644318580627
########
Epoch: 74
Meta Train Loss: 0.6208405494689941
########
Epoch: 75
Meta Train Loss: 1.147777795791626
########
Epoch: 76
Meta Train Loss: 0.6670347452163696
########
Epoch: 77
Meta Train Loss: 0.5073390007019043
########
Epoch: 78
Meta Train Loss: 0.5231288075447083
########
Epoch: 79
Meta Train Loss: 0.6237370371818542
########
Epoch: 80
Meta Train Loss: 1.0421624183654785
########
Epoch: 81
Meta Train Loss: 0.7340062856674194
########
Epoch: 82
Meta Train Loss: 0.6679703593254089
########
Epoch: 83
Meta Train Loss: 0.8189720511436462
########
Epoch: 84
Meta Train Loss: 0.5918952822685242
########
Epoch: 85
Meta Train Loss: 0.6310752630233765
########
Epoch: 86
Meta Train Loss: 1.0167614221572876
########
Epoch: 87
Meta Train Loss: 0.700573742389679
########
Epoch: 88
Meta Train Loss: 0.8522202968597412
########
Epoch: 89
Meta Train Loss: 0.5293055176734924
########
Epoch: 90
Meta Train Loss: 0.6975449323654175
########
Epoch: 91
Meta Train Loss: 0.5512322187423706
########
Epoch: 92
Meta Train Loss: 0.9160990118980408
########
Epoch: 93
Meta Train Loss: 0.855285108089447
########
Epoch: 94
Meta Train Loss: 0.6276293396949768
########
Epoch: 95
Meta Train Loss: 0.716268002986908
########
Epoch: 96
Meta Train Loss: 0.5466275215148926
########
Epoch: 97
Meta Train Loss: 0.5909200310707092
########
Epoch: 98
Meta Train Loss: 0.659428060054779
########
Epoch: 99
Meta Train Loss: 0.9575849771499634
########
Epoch: 100
Meta Train Loss: 0.6256996393203735
########
Epoch: 101
Meta Train Loss: 0.6997984051704407
########
Epoch: 102
Meta Train Loss: 0.541617214679718
########
Epoch: 103
Meta Train Loss: 0.44512009620666504
########
Epoch: 104
Meta Train Loss: 0.5501983165740967
########
Epoch: 105
Meta Train Loss: 0.8151763081550598
########
Epoch: 106
Meta Train Loss: 0.5045902729034424
########
Epoch: 107
Meta Train Loss: 0.6549219489097595
########
Epoch: 108
Meta Train Loss: 0.9711934328079224
########
Epoch: 109
Meta Train Loss: 0.7316604256629944
########
Epoch: 110
Meta Train Loss: 0.601759135723114
########
Epoch: 111
Meta Train Loss: 0.6373352408409119
########
Epoch: 112
Meta Train Loss: 0.6140844225883484
########
Epoch: 113
Meta Train Loss: 0.48803433775901794
########
Epoch: 114
Meta Train Loss: 0.4516727030277252
########
Epoch: 115
Meta Train Loss: 0.7756114602088928
########
Epoch: 116
Meta Train Loss: 0.5577417612075806
########
Epoch: 117
Meta Train Loss: 0.5805001258850098
########
Epoch: 118
Meta Train Loss: 0.5033296942710876
########
Epoch: 119
Meta Train Loss: 0.5141907930374146
########
Epoch: 120
Meta Train Loss: 0.6396858096122742
########
Epoch: 121
Meta Train Loss: 0.6185228824615479
########
Epoch: 122
Meta Train Loss: 0.7121462821960449
########
Epoch: 123
Meta Train Loss: 0.7802876830101013
########
Epoch: 124
Meta Train Loss: 0.6100789904594421
########
Epoch: 125
Meta Train Loss: 0.560737133026123
########
Epoch: 126
Meta Train Loss: 0.5931340456008911
########
Epoch: 127
Meta Train Loss: 0.473521888256073
########
Epoch: 128
Meta Train Loss: 1.080759048461914
########
Epoch: 129
Meta Train Loss: 0.49966469407081604
########

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 10531025: <METALEARN> in cluster <dcc> Exited

Job <METALEARN> was submitted from host <gbarlogin1> by user <tfehjo> in cluster <dcc> at Wed Sep 29 16:17:20 2021
Job was executed on host(s) <n-62-20-12>, in queue <gpuv100>, as user <tfehjo> in cluster <dcc> at Wed Sep 29 16:17:21 2021
</zhome/2b/7/117471> was used as the home directory.
</zhome/2b/7/117471/Thesis/train_scripts> was used as the working directory.
Started at Wed Sep 29 16:17:21 2021
Terminated at Wed Sep 29 17:14:25 2021
Results reported at Wed Sep 29 17:14:25 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
#BSUB -J METALEARN #The name the job will get
#BSUB -q gpuv100 #The queue the job will be committed to, here the GPU enabled queue
#BSUB -gpu "num=1:mode=exclusive_process" #How the job will be run on the VM, here I request 1 GPU with exclusive access i.e. only my c #BSUB -n 1 How many CPU cores my job request
#BSUB -W 24:00 #The maximum runtime my job have note that the queuing might enable shorter jobs earlier due to scheduling.
#BSUB -R "span[hosts=1]" #How many nodes the job requests
#BSUB -R "rusage[mem=12GB]" #How much RAM the job should have access to
#BSUB -R "select[gpu32gb]" #For requesting the extra big GPU w. 32GB of VRAM
#BSUB -o logs/OUTPUT.%J #Log file
#BSUB -e logs/ERROR.%J #Error log file
echo "Starting:"

cd ~/Thesis/metalearning
#cd /Users/theisferre/Documents/SPECIALE/Thesis/src/models

source ~/Thesis/venv-thesis/bin/activate

DATA_DIR=/zhome/2b/7/117471/Thesis/data/processed/metalearning
TRAIN_SIZE=0.8
BATCH_TASK_SIZE=8
K_SHOT=5
ADAPTATION_STEPS=5
EPOCHS=500
ADAPT_LR=0.005
META_LR=0.001
EXCLUDE=citibike2014-tripdata-REGION
LOG_DIR=/zhome/2b/7/117471/Thesis/metalearning
HIDDEN_SIZE=46
DROPOUT_P=0.2
NODE_OUT_FEATURES=10


python /zhome/2b/7/117471/Thesis/src/models/train_meta.py --data_dir $DATA_DIR --train_size $TRAIN_SIZE --batch_task_size $BATCH_TASK_SIZE \
--k_shot $K_SHOT --adaptation_steps $ADAPTATION_STEPS --epochs $EPOCHS --adapt_lr $ADAPT_LR --meta_lr $META_LR --log_dir $LOG_DIR --exclude $EXCLUDE \
--hidden_size $HIDDEN_SIZE --dropout_p $DROPOUT_P --node_out_features $NODE_OUT_FEATURES --gpu






------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   3409.53 sec.
    Max Memory :                                 3879 MB
    Average Memory :                             3794.30 MB
    Total Requested Memory :                     12288.00 MB
    Delta Memory :                               8409.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   3458 sec.
    Turnaround time :                            3425 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/ERROR.10531025> for stderr output of this job.

