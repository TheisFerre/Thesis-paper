INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning_augmented/DATA-HOUR1/TLC2018-FHV-aug-HOUR1-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8054005354642868
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8029246479272842
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7892067283391953
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7853252291679382
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7886965572834015
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7938589006662369
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7927833199501038
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7900074124336243
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7843132019042969
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7882281392812729
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8104124516248703
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.788274273276329
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7913351356983185
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8064791560173035
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.81040158867836
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8091657161712646
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7903059273958206
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7988529205322266
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7823791354894638
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8090658783912659
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8060895353555679
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.799792617559433
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7800696939229965
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7835681736469269
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7976034283638
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.809365838766098
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8092182725667953
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8025188148021698
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8086043298244476
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7904940992593765
INFO:__main__:Finetuned loss: 0.3294188156723976
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.754342794418335
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7859592884778976
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6833240538835526
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6457658410072327
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8156231641769409
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7636275142431259
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.673493891954422
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7308639287948608
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.669907420873642
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7417634129524231
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6382497176527977
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7163573801517487
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.699417918920517
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7341600954532623
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7219119220972061
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6174606680870056
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.758247897028923
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7055223435163498
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7121187001466751
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6845776289701462
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7114321291446686
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6906997561454773
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6982499957084656
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7143402099609375
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6422383189201355
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8149999231100082
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.718755304813385
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7046046555042267
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.70936219394207
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6640141159296036
INFO:__main__:Finetuned loss: 0.3294188156723976
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.51154475659132
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5351038575172424
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4876948446035385
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4292890280485153
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.450119249522686
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.45134346187114716
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4285956397652626
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5476072281599045
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6546682268381119
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5382701307535172
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5182204768061638
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.47924743592739105
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.45184988528490067
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6278321295976639
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.49794598668813705
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4591690003871918
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4584415629506111
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.53860904276371
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4756947010755539
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4759167656302452
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5463489145040512
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.45589808374643326
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.454987995326519
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7042860388755798
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.46259231120347977
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.44863150268793106
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.46926140785217285
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7123405486345291
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4716281294822693
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5257915779948235
INFO:__main__:Finetuned loss: 0.3294188156723976
Traceback (most recent call last):
  File "/zhome/2b/7/117471/Thesis/src/models/compare_metalearning.py", line 373, in <module>
    finetuned_transfer, optimizer_transfer = load_transfer_model(metamodel_abs_path, data_type=TYPE)
  File "/zhome/2b/7/117471/Thesis/src/models/compare_metalearning.py", line 164, in load_transfer_model
    edgeconv_transfer_state_dict = torch.load(f"{path}/finetuned_{data_type}_vanilla_model.pth", map_location=torch.device('cpu'))
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/torch/serialization.py", line 594, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/zhome/2b/7/117471/Thesis/metalearning/NOT-HOUR1/2021-10-20T12:25:02.224325/finetuned_REGION_vanilla_model.pth'
