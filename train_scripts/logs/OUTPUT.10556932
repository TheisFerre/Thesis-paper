Starting:
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Epoch: 1
Meta Train Loss: 0.9927528500556946
########
Epoch: 2
Meta Train Loss: 1.1921775341033936
########
Epoch: 3
Meta Train Loss: 0.9917775988578796
########
Epoch: 4
Meta Train Loss: 0.9771097898483276
########
Epoch: 5
Meta Train Loss: 0.9433567523956299
########
Epoch: 6
Meta Train Loss: 1.2203035354614258
########
Epoch: 7
Meta Train Loss: 33.43962860107422
########
Epoch: 8
Meta Train Loss: 0.8131515383720398
########
Epoch: 9
Meta Train Loss: 0.9209820032119751
########
Epoch: 10
Meta Train Loss: 1.0494929552078247
########
Epoch: 11
Meta Train Loss: 0.9453156590461731
########
Epoch: 12
Meta Train Loss: 1.261919379234314
########
Epoch: 13
Meta Train Loss: 0.8936493992805481
########
Epoch: 14
Meta Train Loss: 1.0532909631729126
########
Epoch: 15
Meta Train Loss: 1.308606505393982
########
Epoch: 16
Meta Train Loss: 0.9795973896980286
########
Epoch: 17
Meta Train Loss: 0.9159209132194519
########
Epoch: 18
Meta Train Loss: 16.970149993896484
########
Epoch: 19
Meta Train Loss: 1.028303861618042
########
Epoch: 20
Meta Train Loss: 1.0553277730941772
########
Epoch: 21
Meta Train Loss: 1.3654026985168457
########
Epoch: 22
Meta Train Loss: 1.1585613489151
########
Epoch: 23
Meta Train Loss: 0.9148663282394409
########
Epoch: 24
Meta Train Loss: 1.0206530094146729
########
Epoch: 25
Meta Train Loss: 1.0539792776107788
########
Epoch: 26
Meta Train Loss: 0.9627876281738281
########
Epoch: 27
Meta Train Loss: 0.9182626605033875
########
Epoch: 28
Meta Train Loss: 1.090406060218811
########
Epoch: 29
Meta Train Loss: 1.094862937927246
########
Epoch: 30
Meta Train Loss: 1.0193231105804443
########
Epoch: 31
Meta Train Loss: 1432.5284423828125
########
Epoch: 32
Meta Train Loss: 1.214526891708374
########
Epoch: 33
Meta Train Loss: 0.9881306290626526
########
Epoch: 34
Meta Train Loss: 1581.8548583984375
########
Epoch: 35
Meta Train Loss: 1.247884750366211
########
Epoch: 36
Meta Train Loss: 1.4479329586029053
########
Epoch: 37
Meta Train Loss: 0.8881765604019165
########
Epoch: 38
Meta Train Loss: 1.0317118167877197
########
Epoch: 39
Meta Train Loss: 8069482.0
########
Epoch: 40
Meta Train Loss: 1.305369257926941
########
Epoch: 41
Meta Train Loss: 0.8572694063186646
########
Epoch: 42
Meta Train Loss: 1.2396214008331299
########
Epoch: 43
Meta Train Loss: 1.0396454334259033
########
Epoch: 44
Meta Train Loss: 1.1370099782943726
########
Epoch: 45
Meta Train Loss: 1.0430035591125488
########
Epoch: 46
Meta Train Loss: 1.1381598711013794
########
Epoch: 47
Meta Train Loss: 1.0842112302780151
########
Epoch: 48
Meta Train Loss: 1.0833990573883057
########
Epoch: 49
Meta Train Loss: 16.891990661621094
########
Epoch: 50
Meta Train Loss: 2.6905646324157715
########
Epoch: 51
Meta Train Loss: 0.736078679561615
########
Epoch: 52
Meta Train Loss: 193.41770935058594
########
Epoch: 53
Meta Train Loss: 0.8962152004241943
########
Epoch: 54
Meta Train Loss: 0.8211402297019958
########
Epoch: 55
Meta Train Loss: 0.7770822644233704
########
Epoch: 56
Meta Train Loss: 1.0680820941925049
########
Epoch: 57
Meta Train Loss: 0.777851939201355
########
Epoch: 58
Meta Train Loss: 24100030464.0
########
Epoch: 59
Meta Train Loss: 0.8192344307899475
########
Epoch: 60
Meta Train Loss: 1.0768216848373413
########
Epoch: 61
Meta Train Loss: 0.9542865753173828
########
Epoch: 62
Meta Train Loss: 453765945622528.0
########
Epoch: 63
Meta Train Loss: 0.7938035726547241
########
Epoch: 64
Meta Train Loss: 0.932877779006958
########
Epoch: 65
Meta Train Loss: 0.8381632566452026
########
Epoch: 66
Meta Train Loss: 1.0028084516525269
########
Epoch: 67
Meta Train Loss: 28866.865234375
########
Epoch: 68
Meta Train Loss: 0.7410151958465576
########
Epoch: 69
Meta Train Loss: 0.8718156814575195
########
Epoch: 70
Meta Train Loss: 0.7109020948410034
########
Epoch: 71
Meta Train Loss: 13.666143417358398
########
Epoch: 72
Meta Train Loss: 0.9774243235588074
########
Epoch: 73
Meta Train Loss: 1.2644685506820679
########
Epoch: 74
Meta Train Loss: 126543.0703125
########
Epoch: 75
Meta Train Loss: 0.742215096950531
########
Epoch: 76
Meta Train Loss: 0.6584169268608093
########
Epoch: 77
Meta Train Loss: 0.6557859182357788
########
Epoch: 78
Meta Train Loss: 0.7421995997428894
########
Epoch: 79
Meta Train Loss: 0.7678318619728088
########
Epoch: 80
Meta Train Loss: 0.9313432574272156
########
Epoch: 81
Meta Train Loss: 0.8386582136154175
########
Epoch: 82
Meta Train Loss: 0.6417638659477234
########
Epoch: 83
Meta Train Loss: 0.661760151386261
########
Epoch: 84
Meta Train Loss: 0.8780820965766907
########
Epoch: 85
Meta Train Loss: 0.8822402954101562
########
Epoch: 86
Meta Train Loss: 0.7190147638320923
########
Epoch: 87
Meta Train Loss: 0.7795198559761047
########
Epoch: 88
Meta Train Loss: 3598.78271484375
########

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 10556932: <METALEARN> in cluster <dcc> Exited

Job <METALEARN> was submitted from host <gbarlogin1> by user <tfehjo> in cluster <dcc> at Mon Oct  4 13:14:07 2021
Job was executed on host(s) <n-62-11-16>, in queue <gpuv100>, as user <tfehjo> in cluster <dcc> at Mon Oct  4 13:14:09 2021
</zhome/2b/7/117471> was used as the home directory.
</zhome/2b/7/117471/Thesis/train_scripts> was used as the working directory.
Started at Mon Oct  4 13:14:09 2021
Terminated at Mon Oct  4 13:43:30 2021
Results reported at Mon Oct  4 13:43:30 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
#BSUB -J METALEARN #The name the job will get
#BSUB -q gpuv100 #The queue the job will be committed to, here the GPU enabled queue
#BSUB -gpu "num=1:mode=exclusive_process" #How the job will be run on the VM, here I request 1 GPU with exclusive access i.e. only my c #BSUB -n 1 How many CPU cores my job request
#BSUB -W 24:00 #The maximum runtime my job have note that the queuing might enable shorter jobs earlier due to scheduling.
#BSUB -R "span[hosts=1]" #How many nodes the job requests
#BSUB -R "rusage[mem=12GB]" #How much RAM the job should have access to
#BSUB -R "select[gpu32gb]" #For requesting the extra big GPU w. 32GB of VRAM
#BSUB -o logs/OUTPUT.%J #Log file
#BSUB -e logs/ERROR.%J #Error log file
echo "Starting:"

cd ~/Thesis/metalearning
#cd /Users/theisferre/Documents/SPECIALE/Thesis/src/models

source ~/Thesis/venv-thesis/bin/activate

DATA_DIR=/zhome/2b/7/117471/Thesis/data/processed/metalearning
TRAIN_SIZE=0.9
BATCH_TASK_SIZE=-1
K_SHOT=5
ADAPTATION_STEPS=10
EPOCHS=250
ADAPT_LR=0.1
META_LR=0.001
EXCLUDE=citibike-tripdata
LOG_DIR=/zhome/2b/7/117471/Thesis/metalearning
HIDDEN_SIZE=46
DROPOUT_P=0.2
NODE_OUT_FEATURES=10


python /zhome/2b/7/117471/Thesis/src/models/train_meta.py --data_dir $DATA_DIR --train_size $TRAIN_SIZE --batch_task_size $BATCH_TASK_SIZE \
--k_shot $K_SHOT --adaptation_steps $ADAPTATION_STEPS --epochs $EPOCHS --adapt_lr $ADAPT_LR --meta_lr $META_LR --log_dir $LOG_DIR --exclude $EXCLUDE \
--hidden_size $HIDDEN_SIZE --dropout_p $DROPOUT_P --node_out_features $NODE_OUT_FEATURES --gpu






------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   1751.62 sec.
    Max Memory :                                 3416 MB
    Average Memory :                             3192.86 MB
    Total Requested Memory :                     12288.00 MB
    Delta Memory :                               8872.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   1761 sec.
    Turnaround time :                            1763 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/ERROR.10556932> for stderr output of this job.

