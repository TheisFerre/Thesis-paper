INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/yellow-taxi2020-nov-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.9624219536781311
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9490387588739395
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9856191426515579
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9787754863500595
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9550872296094894
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9863694608211517
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.955690324306488
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9855901449918747
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9447164237499237
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9760764986276627
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9742795526981354
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9394729435443878
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.983353927731514
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9518269151449203
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9529054909944534
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9552457183599472
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9404080957174301
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9472476243972778
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9245485663414001
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9553672820329666
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9541837871074677
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.934611514210701
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9715766310691833
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9605446755886078
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9653715044260025
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.943177655339241
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9468304961919785
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9459314793348312
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9621323198080063
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: 0.9709807187318802
INFO:__main__:Finetuned loss: 0.7760983854532242
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.7760983854532242
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
Traceback (most recent call last):
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/learn2learn/algorithms/maml.py", line 159, in adapt
    gradients = grad(loss,
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/torch/autograd/__init__.py", line 226, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.75 GiB total capacity; 29.02 GiB already allocated; 9.50 MiB free; 30.39 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/zhome/2b/7/117471/Thesis/src/models/compare_metalearning.py", line 391, in <module>
    losses = eval_models(
  File "/zhome/2b/7/117471/Thesis/src/models/compare_metalearning.py", line 270, in eval_models
    learner.adapt(support_loss)
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/learn2learn/algorithms/maml.py", line 169, in adapt
    self.module = maml_update(self.module, self.lr, gradients)
UnboundLocalError: local variable 'gradients' referenced before assignment
