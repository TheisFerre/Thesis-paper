Starting:
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Epoch: 1
Meta Train Loss: 1.1808366775512695
########
Epoch: 2
Meta Train Loss: 1.021008849143982
########
Epoch: 3
Meta Train Loss: 1.012026309967041
########
Epoch: 4
Meta Train Loss: 0.8735924959182739
########
Epoch: 5
Meta Train Loss: 1.2180426120758057
########
Epoch: 6
Meta Train Loss: 1.1972981691360474
########
Epoch: 7
Meta Train Loss: 0.8603858351707458
########
Epoch: 8
Meta Train Loss: 0.9527537226676941
########
Epoch: 9
Meta Train Loss: 0.9022594690322876
########
Epoch: 10
Meta Train Loss: 0.9590268135070801
########
Epoch: 11
Meta Train Loss: 0.9885365962982178
########
Epoch: 12
Meta Train Loss: 1.1777865886688232
########
Epoch: 13
Meta Train Loss: 1.179478645324707
########
Epoch: 14
Meta Train Loss: 1.2321135997772217
########
Epoch: 15
Meta Train Loss: 1.0104271173477173
########
Epoch: 16
Meta Train Loss: 1.1621465682983398
########
Epoch: 17
Meta Train Loss: 0.9137442111968994
########
Epoch: 18
Meta Train Loss: 0.8865252733230591
########
Epoch: 19
Meta Train Loss: 0.9473802447319031
########
Epoch: 20
Meta Train Loss: 1.03842294216156
########
Epoch: 21
Meta Train Loss: 1.1148796081542969
########
Epoch: 22
Meta Train Loss: 1.0996828079223633
########
Epoch: 23
Meta Train Loss: 0.9132177829742432
########
Epoch: 24
Meta Train Loss: 0.9936472177505493
########
Epoch: 25
Meta Train Loss: 1.0157036781311035
########
Epoch: 26
Meta Train Loss: 0.8577895164489746
########
Epoch: 27
Meta Train Loss: 0.7470809817314148
########
Epoch: 28
Meta Train Loss: 0.9064052104949951
########
Epoch: 29
Meta Train Loss: 1.2439461946487427
########
Epoch: 30
Meta Train Loss: 1.0661956071853638
########
Epoch: 31
Meta Train Loss: 0.7713578939437866
########
Epoch: 32
Meta Train Loss: 1.366155982017517
########
Epoch: 33
Meta Train Loss: 0.7612647414207458
########
Epoch: 34
Meta Train Loss: 0.9517633318901062
########
Epoch: 35
Meta Train Loss: 0.9604074954986572
########
Epoch: 36
Meta Train Loss: 1.2193794250488281
########
Epoch: 37
Meta Train Loss: 0.7968277931213379
########
Epoch: 38
Meta Train Loss: 0.7736074924468994
########
Epoch: 39
Meta Train Loss: 0.5176295042037964
########
Epoch: 40
Meta Train Loss: 0.8794167041778564
########
Epoch: 41
Meta Train Loss: 0.962380588054657
########
Epoch: 42
Meta Train Loss: 0.9150010943412781
########
Epoch: 43
Meta Train Loss: 0.7624573707580566
########
Epoch: 44
Meta Train Loss: 0.6478845477104187
########
Epoch: 45
Meta Train Loss: 0.6775833368301392
########
Epoch: 46
Meta Train Loss: 0.5292324423789978
########
Epoch: 47
Meta Train Loss: 0.8783221244812012
########
Epoch: 48
Meta Train Loss: 0.8146397471427917
########
Epoch: 49
Meta Train Loss: 0.5815245509147644
########
Epoch: 50
Meta Train Loss: 0.8118637800216675
########
Epoch: 51
Meta Train Loss: 0.707290768623352
########
Epoch: 52
Meta Train Loss: 0.628091037273407
########
Epoch: 53
Meta Train Loss: 0.6948243975639343
########
Epoch: 54
Meta Train Loss: 0.5765998363494873
########
Epoch: 55
Meta Train Loss: 0.7787601947784424
########
Epoch: 56
Meta Train Loss: 0.5887124538421631
########
Epoch: 57
Meta Train Loss: 0.5846837759017944
########
Epoch: 58
Meta Train Loss: 0.961157500743866
########
Epoch: 59
Meta Train Loss: 0.5562018156051636
########
Epoch: 60
Meta Train Loss: 0.7551838755607605
########
Epoch: 61
Meta Train Loss: 0.4503327012062073
########
Epoch: 62
Meta Train Loss: 0.8632279634475708
########
Epoch: 63
Meta Train Loss: 0.5802146792411804
########
Epoch: 64
Meta Train Loss: 0.6655595302581787
########
Epoch: 65
Meta Train Loss: 0.7318493723869324
########
Epoch: 66
Meta Train Loss: 0.571977972984314
########
Epoch: 67
Meta Train Loss: 0.7177108526229858
########
Epoch: 68
Meta Train Loss: 0.5157081484794617
########
Epoch: 69
Meta Train Loss: 0.5982342958450317
########
Epoch: 70
Meta Train Loss: 0.6324867010116577
########
Epoch: 71
Meta Train Loss: 0.5370866060256958
########
Epoch: 72
Meta Train Loss: 0.5319734811782837
########
Epoch: 73
Meta Train Loss: 0.6150131821632385
########
Epoch: 74
Meta Train Loss: 0.6706255674362183
########
Epoch: 75
Meta Train Loss: 0.6765992045402527
########
Epoch: 76
Meta Train Loss: 0.5393257141113281
########
Epoch: 77
Meta Train Loss: 0.5698479413986206
########
Epoch: 78
Meta Train Loss: 0.4552890658378601
########
Epoch: 79
Meta Train Loss: 0.5852317214012146
########
Epoch: 80
Meta Train Loss: 0.6775591373443604
########
Epoch: 81
Meta Train Loss: 0.5550678968429565
########
Epoch: 82
Meta Train Loss: 0.5327228307723999
########
Epoch: 83
Meta Train Loss: 0.8310768008232117
########
Epoch: 84
Meta Train Loss: 0.4564824104309082
########
Epoch: 85
Meta Train Loss: 0.5326039791107178
########
Epoch: 86
Meta Train Loss: 0.5858317017555237
########
Epoch: 87
Meta Train Loss: 1.136636734008789
########
Epoch: 88
Meta Train Loss: 0.6021352410316467
########
Epoch: 89
Meta Train Loss: 0.5002902150154114
########
Epoch: 90
Meta Train Loss: 0.5606718063354492
########
Epoch: 91
Meta Train Loss: 0.4231075644493103
########
Epoch: 92
Meta Train Loss: 0.47646889090538025
########
Epoch: 93
Meta Train Loss: 0.6153093576431274
########
Epoch: 94
Meta Train Loss: 0.4658745229244232
########
Epoch: 95
Meta Train Loss: 0.8341960310935974
########
Epoch: 96
Meta Train Loss: 0.6295371651649475
########
Epoch: 97
Meta Train Loss: 0.5369691252708435
########
Epoch: 98
Meta Train Loss: 0.5620818138122559
########
Epoch: 99
Meta Train Loss: 0.49680036306381226
########
Epoch: 100
Meta Train Loss: 0.45821481943130493
########
Epoch: 101
Meta Train Loss: 0.5408151149749756
########
Epoch: 102
Meta Train Loss: 0.5630456209182739
########
Epoch: 103
Meta Train Loss: 0.6364742517471313
########
Epoch: 104
Meta Train Loss: 0.46545884013175964
########
Epoch: 105
Meta Train Loss: 0.5187698006629944
########
Epoch: 106
Meta Train Loss: 0.5835155248641968
########
Epoch: 107
Meta Train Loss: 0.5473952293395996
########
Epoch: 108
Meta Train Loss: 0.6141062378883362
########
Epoch: 109
Meta Train Loss: 0.48991113901138306
########
Epoch: 110
Meta Train Loss: 0.46219274401664734
########
Epoch: 111
Meta Train Loss: 0.6085339784622192
########
Epoch: 112
Meta Train Loss: 0.621193528175354
########
Epoch: 113
Meta Train Loss: 0.45656368136405945
########
Epoch: 114
Meta Train Loss: 0.6970421671867371
########
Epoch: 115
Meta Train Loss: 0.5532878637313843
########
Epoch: 116
Meta Train Loss: 0.7134473323822021
########
Epoch: 117
Meta Train Loss: 0.6613231301307678
########
Epoch: 118
Meta Train Loss: 0.49549412727355957
########
Epoch: 119
Meta Train Loss: 0.5419078469276428
########
Epoch: 120
Meta Train Loss: 0.5157426595687866
########
Epoch: 121
Meta Train Loss: 0.5184513926506042
########
Epoch: 122
Meta Train Loss: 0.538413941860199
########
Epoch: 123
Meta Train Loss: 0.7006138563156128
########
Epoch: 124
Meta Train Loss: 0.7557532787322998
########
Epoch: 125
Meta Train Loss: 0.7574906349182129
########
Epoch: 126
Meta Train Loss: 0.4621141850948334
########
Epoch: 127
Meta Train Loss: 0.5856996774673462
########
Epoch: 128
Meta Train Loss: 0.7324070930480957
########
Epoch: 129
Meta Train Loss: 0.9170181155204773
########
Epoch: 130
Meta Train Loss: 0.6559072732925415
########
Epoch: 131
Meta Train Loss: 0.5906901359558105
########
Epoch: 132
Meta Train Loss: 0.8078160285949707
########
Epoch: 133
Meta Train Loss: 0.5601966381072998
########
Epoch: 134
Meta Train Loss: 0.544808566570282
########
Epoch: 135
Meta Train Loss: 0.6204541921615601
########
Epoch: 136
Meta Train Loss: 0.6543457508087158
########
Epoch: 137
Meta Train Loss: 0.7140922546386719
########
Epoch: 138
Meta Train Loss: 0.3740672469139099
########
Epoch: 139
Meta Train Loss: 0.43267759680747986
########
Epoch: 140
Meta Train Loss: 0.6133319139480591
########
Epoch: 141
Meta Train Loss: 0.5360172986984253
########
Epoch: 142
Meta Train Loss: 0.5857304334640503
########
Epoch: 143
Meta Train Loss: 0.5468484163284302
########
Epoch: 144
Meta Train Loss: 0.5081668496131897
########
Epoch: 145
Meta Train Loss: 0.5625704526901245
########
Epoch: 146
Meta Train Loss: 0.4725293815135956
########
Epoch: 147
Meta Train Loss: 1.779566764831543
########
Epoch: 148
Meta Train Loss: 0.5220343470573425
########
Epoch: 149
Meta Train Loss: 0.5236403346061707
########
Epoch: 150
Meta Train Loss: 0.5505915880203247
########
Epoch: 151
Meta Train Loss: 0.5225415229797363
########
Epoch: 152
Meta Train Loss: 0.3745672106742859
########
Epoch: 153
Meta Train Loss: 0.45260700583457947
########
Epoch: 154
Meta Train Loss: 0.513044536113739
########
Epoch: 155
Meta Train Loss: 0.6910077929496765
########
Epoch: 156
Meta Train Loss: 0.36730265617370605
########
Epoch: 157
Meta Train Loss: 0.7028619647026062
########
Epoch: 158
Meta Train Loss: 0.5909345149993896
########
Epoch: 159
Meta Train Loss: 0.636479377746582
########
Epoch: 160
Meta Train Loss: 0.6520748138427734
########
Epoch: 161
Meta Train Loss: 0.5390686392784119
########
Epoch: 162
Meta Train Loss: 0.4781726598739624
########
Epoch: 163
Meta Train Loss: 0.5332010388374329
########
Epoch: 164
Meta Train Loss: 0.5321267247200012
########
Epoch: 165
Meta Train Loss: 0.4845196604728699
########
Epoch: 166
Meta Train Loss: 0.676550567150116
########
Epoch: 167
Meta Train Loss: 0.41282910108566284
########
Epoch: 168
Meta Train Loss: 0.8840571641921997
########
Epoch: 169
Meta Train Loss: 0.5858755111694336
########
Epoch: 170
Meta Train Loss: 0.4621821343898773
########
Epoch: 171
Meta Train Loss: 0.409536212682724
########
Epoch: 172
Meta Train Loss: 0.4767063856124878
########
Epoch: 173
Meta Train Loss: 0.4255281090736389
########
Epoch: 174
Meta Train Loss: 0.5710388422012329
########
Epoch: 175
Meta Train Loss: 0.6145901083946228
########
Epoch: 176
Meta Train Loss: 0.5298431515693665
########
Epoch: 177
Meta Train Loss: 0.7039041519165039
########
Epoch: 178
Meta Train Loss: 0.8252449035644531
########
Epoch: 179
Meta Train Loss: 0.5550844073295593
########
Epoch: 180
Meta Train Loss: 0.5747931003570557
########
Epoch: 181
Meta Train Loss: 0.4882914423942566
########
Epoch: 182
Meta Train Loss: 0.5626838803291321
########
Epoch: 183
Meta Train Loss: 0.4682668149471283
########
Epoch: 184
Meta Train Loss: 0.651890218257904
########
Epoch: 185
Meta Train Loss: 0.510695219039917
########
Epoch: 186
Meta Train Loss: 0.615158200263977
########
Epoch: 187
Meta Train Loss: 0.5448120832443237
########
Epoch: 188
Meta Train Loss: 0.68681800365448
########
Epoch: 189
Meta Train Loss: 0.5940383076667786
########
Epoch: 190
Meta Train Loss: 0.539827287197113
########
Epoch: 191
Meta Train Loss: 0.5260589122772217
########
Epoch: 192
Meta Train Loss: 0.42755529284477234
########
Epoch: 193
Meta Train Loss: 0.5023157596588135
########
Epoch: 194
Meta Train Loss: 0.4418126344680786
########
Epoch: 195
Meta Train Loss: 0.6637909412384033
########
Epoch: 196
Meta Train Loss: 0.5713807940483093
########
Epoch: 197
Meta Train Loss: 0.454415500164032
########
Epoch: 198
Meta Train Loss: 0.5724614858627319
########
Epoch: 199
Meta Train Loss: 0.6500006914138794
########
Epoch: 200
Meta Train Loss: 0.6646361351013184
########
Epoch: 201
Meta Train Loss: 0.7094978094100952
########
Epoch: 202
Meta Train Loss: 0.5795800685882568
########
Epoch: 203
Meta Train Loss: 0.6810171008110046
########
Epoch: 204
Meta Train Loss: 0.4159986674785614
########
Epoch: 205
Meta Train Loss: 0.4877888858318329
########
Epoch: 206
Meta Train Loss: 0.4805498719215393
########
Epoch: 207
Meta Train Loss: 194237088.0
########
Epoch: 208
Meta Train Loss: 0.5937327742576599
########
Epoch: 209
Meta Train Loss: 0.44082412123680115
########
Epoch: 210
Meta Train Loss: 0.5418627858161926
########
Epoch: 211
Meta Train Loss: 0.6240931153297424
########
Epoch: 212
Meta Train Loss: 0.7178642749786377
########
Epoch: 213
Meta Train Loss: 0.680715799331665
########
Epoch: 214
Meta Train Loss: 0.4775678515434265
########
Epoch: 215
Meta Train Loss: 0.8233047723770142
########
Epoch: 216
Meta Train Loss: 0.6596585512161255
########
Epoch: 217
Meta Train Loss: 0.5648917555809021
########
Epoch: 218
Meta Train Loss: 0.5804747343063354
########
Epoch: 219
Meta Train Loss: 0.47586357593536377
########
Epoch: 220
Meta Train Loss: 0.7455066442489624
########
Epoch: 221
Meta Train Loss: 0.43963372707366943
########
Epoch: 222
Meta Train Loss: 0.5042015314102173
########
Epoch: 223
Meta Train Loss: 0.6669217944145203
########
Epoch: 224
Meta Train Loss: 0.5903657674789429
########
Epoch: 225
Meta Train Loss: 0.5425057411193848
########
Epoch: 226
Meta Train Loss: 0.43623650074005127
########
Epoch: 227
Meta Train Loss: 0.5903605222702026
########
Epoch: 228
Meta Train Loss: 23857294.0
########
Epoch: 229
Meta Train Loss: 0.4364728629589081
########
Epoch: 230
Meta Train Loss: 0.4949997365474701
########
Epoch: 231
Meta Train Loss: 0.709155261516571
########
Epoch: 232
Meta Train Loss: 0.5831325054168701
########
Epoch: 233
Meta Train Loss: 0.7102028727531433
########
Epoch: 234
Meta Train Loss: 0.3827904164791107
########
Epoch: 235
Meta Train Loss: 0.6529548168182373
########
Epoch: 236
Meta Train Loss: 0.521183967590332
########
Epoch: 237
Meta Train Loss: 0.5736626386642456
########
Epoch: 238
Meta Train Loss: 0.5454177260398865
########
Epoch: 239
Meta Train Loss: 2015042.875
########
Epoch: 240
Meta Train Loss: 0.8604256510734558
########
Epoch: 241
Meta Train Loss: 0.5124318599700928
########
Epoch: 242
Meta Train Loss: 0.6182540059089661
########
Epoch: 243
Meta Train Loss: 0.5820412635803223
########
Epoch: 244
Meta Train Loss: 0.5422446727752686
########
Epoch: 245
Meta Train Loss: 0.5593140721321106
########
Epoch: 246
Meta Train Loss: 0.5225658416748047
########
Epoch: 247
Meta Train Loss: 61968740352.0
########
Epoch: 248
Meta Train Loss: 0.6762194037437439
########
Epoch: 249
Meta Train Loss: 0.4486786127090454
########
Epoch: 250
Meta Train Loss: 0.5699857473373413
########

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 11096670: <METALEARN> in cluster <dcc> Done

Job <METALEARN> was submitted from host <gbarlogin1> by user <tfehjo> in cluster <dcc> at Thu Nov 18 15:03:29 2021
Job was executed on host(s) <n-62-20-14>, in queue <gpuv100>, as user <tfehjo> in cluster <dcc> at Sun Nov 21 06:28:47 2021
</zhome/2b/7/117471> was used as the home directory.
</zhome/2b/7/117471/Thesis/train_scripts> was used as the working directory.
Started at Sun Nov 21 06:28:47 2021
Terminated at Sun Nov 21 07:49:12 2021
Results reported at Sun Nov 21 07:49:12 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
#BSUB -J METALEARN #The name the job will get
#BSUB -q gpuv100 #The queue the job will be committed to, here the GPU enabled queue
#BSUB -gpu "num=1:mode=exclusive_process" #How the job will be run on the VM, here I request 1 GPU with exclusive access i.e. only my c #BSUB -n 1 How many CPU cores my job request
#BSUB -W 24:00 #The maximum runtime my job have note that the queuing might enable shorter jobs earlier due to scheduling.
#BSUB -R "span[hosts=1]" #How many nodes the job requests
#BSUB -R "rusage[mem=12GB]" #How much RAM the job should have access to
#BSUB -R "select[gpu32gb]" #For requesting the extra big GPU w. 32GB of VRAM
#BSUB -o logs/OUTPUT.%J #Log file
#BSUB -e logs/ERROR.%J #Error log file
echo "Starting:"

cd ~/Thesis/metalearning
#cd /Users/theisferre/Documents/SPECIALE/Thesis/src/models

source ~/Thesis/venv-thesis/bin/activate

DATA_DIR=/zhome/2b/7/117471/Thesis/data/processed/aglation-non_augmented
TRAIN_SIZE=0.9
BATCH_TASK_SIZE=10
K_SHOT=5
ADAPTATION_STEPS=10
EPOCHS=250
ADAPT_LR=0.05
META_LR=0.001
EXCLUDE=citibike-tripdata-GRID,TLC2018-REGION,citibike2014-GRID,GM,yellow-taxi2020-nov-REGION,green,UBER2015-jan-june-GRID,LYFT,TLC2018-GRID,citibike2014-REGION,UBER2015-jan-june-GRID
LOG_DIR=/zhome/2b/7/117471/Thesis/ablation-study/non-augmented
HIDDEN_SIZE=46
DROPOUT_P=0.2
NODE_OUT_FEATURES=10

# citibike-tripdata-GRID,TLC2018-REGION,citibike2014-GRID,GM,yellow-taxi2020-nov-REGION,green,UBER2015-jan-june-GRID,LYFT,TLC2018-GRID,citibike2014-REGION,UBER2015-jan-june-GRID,yellow-taxi2020-nov-GRID

python /zhome/2b/7/117471/Thesis/src/models/train_meta.py --data_dir $DATA_DIR --train_size $TRAIN_SIZE --batch_task_size $BATCH_TASK_SIZE \
--k_shot $K_SHOT --adaptation_steps $ADAPTATION_STEPS --epochs $EPOCHS --adapt_lr $ADAPT_LR --meta_lr $META_LR --log_dir $LOG_DIR \
--hidden_size $HIDDEN_SIZE --dropout_p $DROPOUT_P --node_out_features $NODE_OUT_FEATURES --exclude $EXCLUDE --gpu


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4654.40 sec.
    Max Memory :                                 3187 MB
    Average Memory :                             3053.03 MB
    Total Requested Memory :                     12288.00 MB
    Delta Memory :                               9101.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                9
    Run time :                                   4825 sec.
    Turnaround time :                            233143 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/ERROR.11096670> for stderr output of this job.

