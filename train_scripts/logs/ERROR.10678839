INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning_augmented/DATA-HOUR1/TLC2018-FHV-aug-HOUR1-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8054005354642868
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8029246479272842
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7892067283391953
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7853252291679382
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7886965572834015
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7938589006662369
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7927833199501038
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7900074124336243
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7843132019042969
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7882281392812729
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8104124516248703
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.788274273276329
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7913351356983185
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8064791560173035
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.81040158867836
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8091657161712646
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7903059273958206
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7988529205322266
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7823791354894638
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8090658783912659
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8060895353555679
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.799792617559433
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7800696939229965
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7835681736469269
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7976034283638
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.809365838766098
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8092182725667953
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8025188148021698
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8086043298244476
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7904940992593765
INFO:__main__:Finetuned loss: 0.3294188156723976
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7543427795171738
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7859592586755753
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6833240538835526
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6457658261060715
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8156231790781021
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7636274993419647
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6734938770532608
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7308639287948608
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6699074357748032
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7417634129524231
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6382497176527977
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7163573801517487
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6994179040193558
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7341601252555847
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7219119369983673
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6174606680870056
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.758247897028923
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7055223435163498
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7121187001466751
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6845776438713074
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7114321142435074
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6906972229480743
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6982499957084656
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7143402099609375
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6422383189201355
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.8149973750114441
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.718755304813385
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7046056538820267
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7093622088432312
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6640141308307648
INFO:__main__:Finetuned loss: 0.3294188156723976
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.5115447416901588
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5351038575172424
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4876895695924759
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4292890280485153
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.450119249522686
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.45134346187114716
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.428595632314682
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5476072281599045
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6546682268381119
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5382701307535172
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5182663947343826
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4792485013604164
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4518497958779335
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.6278321146965027
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.49794598668813705
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4591689854860306
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4584415405988693
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5386090278625488
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.47569477558135986
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4759167805314064
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5463489517569542
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.45589789748191833
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.454987995326519
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7042882293462753
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4625937268137932
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.4486314728856087
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.46926163136959076
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.7123417407274246
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.47166312485933304
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:Meta Train Loss: 0.5257918834686279
INFO:__main__:Finetuned loss: 0.3294188156723976
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning_augmented/DATA-HOUR1/green-taxi2020-dec-HOUR1-GRID10.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8770541548728943
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8925328999757767
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8915470689535141
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8946521878242493
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.90831558406353
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9155805110931396
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9130828380584717
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9147205948829651
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8757071048021317
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8871251344680786
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8980100005865097
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8960763663053513
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.903466671705246
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9119540899991989
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8941398411989212
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8940945565700531
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9027416557073593
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8990336656570435
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8872837871313095
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8926746845245361
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.885510578751564
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8993103355169296
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9018139690160751
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8962437808513641
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9099332690238953
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8776848167181015
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9225034266710281
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9495005011558533
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9102372080087662
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8889845311641693
INFO:__main__:Finetuned loss: 0.6781467944383621
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8743807822465897
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8914040476083755
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8725064098834991
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.928751215338707
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.870158851146698
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8861677199602127
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9176116585731506
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9365067034959793
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9059216380119324
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8666241317987442
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8651322424411774
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9204600900411606
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8923106044530869
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9616426825523376
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8954516500234604
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8585397750139236
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8654269129037857
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9092813581228256
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 2.454855608556836e+22
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9390183985233307
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8563167899847031
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8632820397615433
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8604890704154968
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8400472551584244
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8940299153327942
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9490604400634766
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8724423348903656
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9055196791887283
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9355333149433136
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9207692295312881
INFO:__main__:Finetuned loss: 0.6781467944383621
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8470472395420074
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.792246013879776
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.7849108427762985
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9282917380332947
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.7863159775733948
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9133647978305817
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9195546358823776
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8780435919761658
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8467333167791367
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8688070178031921
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.799821674823761
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.7912875115871429
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8102955371141434
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8824056833982468
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.7536764889955521
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9364476650953293
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8132629990577698
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.7792093008756638
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9018513411283493
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8139868676662445
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8066132664680481
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8491173535585403
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.7869125455617905
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8046733736991882
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9084822237491608
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8117337673902512
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.9231952428817749
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8616099655628204
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.864648848772049
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:Meta Train Loss: 0.8041297048330307
INFO:__main__:Finetuned loss: 0.6781467944383621
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning_augmented/DATA-HOUR1/UBER2015-jan-june-HOUR1-GRID5.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.0029683113098145
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.00880136273124
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.007725095207041
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.006519848650152
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.011928672140295
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0059471021999011
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0046090971339832
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0040883313525806
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.001581457528201
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0014755942604758
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0019681264053693
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0041358958591113
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0028466690670361
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0012122257189318
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0026982480829412
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0072961178692905
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.002075653184544
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0115940895947544
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.004967293956063
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0132745639844374
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0087741125713696
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.01141895218329
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0010938319292935
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0003858723423698
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0180867666547948
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0067019733515652
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0027518516237086
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0033914690667933
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0060784870927983
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0121825310316952
INFO:__main__:Finetuned loss: 0.8225614482706244
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8898152248425917
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.91163139722564
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8963347944346342
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9562802666967566
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9419656748121435
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9719901572574269
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0004963956095956
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9118806936524131
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.905395736748522
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9195969619534232
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0128546086224643
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9514939785003662
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.942182795567946
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9779794812202454
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.916173978285356
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 1.0021688044071198
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.944195572625507
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8920150155370886
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9507517597892068
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9722301851619374
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9221889268268239
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9484389315951954
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8392457393082705
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9604710638523102
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.936157684434544
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8814250420440327
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9043812914328142
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.965717919848182
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8171340443871238
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9018314155665311
INFO:__main__:Finetuned loss: 0.8225614482706244
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8000757802616466
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8273005377162587
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.7406809424812143
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8910972638563677
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8127658854831349
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9016811847686768
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8482828465375033
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8732405955141241
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.7316405258395455
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8367583060806448
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.7337381446903403
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8230871978131208
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8523537489500913
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9147543988444589
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.7917265919121829
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.7397976869886572
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.7463698075576262
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8251370164481077
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9367624629627574
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8525086614218625
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.7659928771582517
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9594477008689534
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.822789343920621
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8443882993676446
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8140456920320337
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.7459492832422256
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9036868024956096
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.9190032861449502
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.7803777727213773
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:Meta Train Loss: 0.8384845636107705
INFO:__main__:Finetuned loss: 0.8225614482706244
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning_augmented/DATA-HOUR1/citibike2014-tripdata-HOUR1-GRID5.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.9140570136633787
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.910439284010367
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9128002009608529
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9135437932881442
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9145736802708019
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9148690321228721
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9148835675282911
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9138148752125826
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9116754965348677
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9121152839877389
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9120670773766257
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9123275225812738
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9130112677812576
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9156816412102092
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.911895521662452
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9128551456061277
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9139671461148695
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9141680936921727
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9129491773518649
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9120388003912839
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9130321388894861
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9106501964005557
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9144841215827249
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9124829742041501
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9156694628975608
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9143289137970317
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9151470877907493
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9104719419370998
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9132042771035974
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9115238419987939
INFO:__main__:Finetuned loss: 0.6364321397109465
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8865427483211864
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 1.3939185630191455
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8613077659498561
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9831716228615154
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8401270305568521
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9421223700046539
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8525673394853418
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.833951171148907
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8864345225420865
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8148318800059232
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8397316878492181
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9201479445804249
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8389179598201405
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9228480106050317
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.7241666059602391
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8016785085201263
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.6855809932405298
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9158723124048926
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9027537703514099
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8125554458661512
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.7669983045621351
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.7848224829543721
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9301980598406359
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9253845038739118
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.7241997122764587
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.9125159355727109
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.7948784340511669
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8360483781857924
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8194118670441888
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.7410939660939303
INFO:__main__:Finetuned loss: 0.6364321397109465
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.4484219483353875
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.6499952226877213
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.46774141084064136
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.49081085283647885
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.4754816700111736
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.4951378107070923
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.5625263547355478
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.5031162459741939
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.6945704303004525
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.497511319138787
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.49204602431167255
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.48692967742681503
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.49907900393009186
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.45325407792221417
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.48357630385593936
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.4748673141002655
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.46372130513191223
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.6119679036465558
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.5709670429879968
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.8042699653993953
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.6522163626822558
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.5030372142791748
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.5737882012670691
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.4905367602001537
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.4985623630610379
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.6354001218622382
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.7396673858165741
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.48224054818803613
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.49732406301931903
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:Meta Train Loss: 0.45665965703400696
INFO:__main__:Finetuned loss: 0.6364321397109465
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning_augmented/DATA-HOUR1/green-taxi2020-dec-HOUR1-GRID5.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8705222606658936
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8672585338354111
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8830777406692505
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8844684809446335
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9005384892225266
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9010278284549713
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9087117910385132
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8970474600791931
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.872726559638977
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8658309727907181
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8798438161611557
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8577785640954971
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8850412517786026
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.877481684088707
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8840860575437546
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8831563740968704
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.865375742316246
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8821998536586761
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8759858310222626
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8807229548692703
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8741636723279953
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8865478783845901
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8845268785953522
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.880151554942131
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8867683261632919
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.873231366276741
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9070777297019958
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9455311000347137
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9154791533946991
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8798852115869522
INFO:__main__:Finetuned loss: 0.9311894476413727
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.828924223780632
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.877104863524437
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8094164207577705
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8947487771511078
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8458355367183685
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8472422510385513
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8677827268838882
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9148582220077515
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8705078363418579
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8520485907793045
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.7843847870826721
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9112939387559891
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8698006868362427
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9552308544516563
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8681282997131348
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8464184552431107
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8621623069047928
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8004704117774963
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: nan
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9436688125133514
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.7590100318193436
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.7373949736356735
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.7887537479400635
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8079275488853455
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.874029666185379
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9565380066633224
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.848293200135231
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9465073943138123
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9187964648008347
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9058397859334946
INFO:__main__:Finetuned loss: 0.9311894476413727
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.74832583963871
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.719441756606102
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.6465151309967041
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8696248978376389
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.676034227013588
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8621166497468948
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8431420773267746
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8167361170053482
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8156197816133499
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8309135735034943
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.6769657954573631
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.6755429655313492
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.7287501394748688
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8047665655612946
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.6407399848103523
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9498116225004196
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.7043656334280968
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.6684662327170372
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8643266558647156
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.6999396532773972
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.6765984743833542
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.7306786626577377
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.6311991214752197
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.6996148079633713
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.8898819535970688
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.6845602095127106
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.9002036154270172
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.7535246163606644
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.7521454989910126
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:Meta Train Loss: 0.7003363221883774
INFO:__main__:Finetuned loss: 0.9311894476413727
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning_augmented/DATA-HOUR1/citibike2014-tripdata-HOUR1-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.9758492139252749
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9745894318277185
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9736499786376953
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9742406010627747
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9766624461520802
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9758197881958701
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9763187603517012
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9769277789375999
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.975921172987331
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9750684851949866
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9750053990970958
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9762603369626132
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9762241081757979
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9782060980796814
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9749225730245764
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9751839177175001
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9760249013250525
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9768980822779916
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9759475074031136
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9730153679847717
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9761590632525358
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9755152247168801
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9764724590561606
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9760272909294475
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9765600047328256
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9764086712490428
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9768213521350514
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9747659862041473
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9758466157046232
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9744865298271179
INFO:__main__:Finetuned loss: 0.5542344450950623
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.003122784874656
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 1.097321783954447
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9461326761679216
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9466366740790281
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9381303895603527
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9315697361122478
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.965565331957557
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9186187874187123
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8414528965950012
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.922725877978585
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9277526207945563
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9152651239525188
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8803801699118181
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9365423863584345
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8674221038818359
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8891225511377508
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8413482958620245
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9858462566679175
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9614719978787682
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8897694403474982
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9001548588275909
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8814019073139537
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8970303968949751
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9798852828415957
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8633621389215643
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9354194511066783
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9168102199381049
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.921467510136691
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.9460469755259427
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8885486694899473
INFO:__main__:Finetuned loss: 0.5542344450950623
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7471624341878024
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8510645248673179
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7336496385661039
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7664529396729036
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7483518760312687
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7633324522863735
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.81051750887524
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7562832344662059
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8876074823466215
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.761579926718365
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7177560749379072
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7583889419382269
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.741418948227709
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7253994995897467
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7319036654450677
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7171613018621098
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7232149162075736
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8138033314184709
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8324785381555557
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8862394053827632
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8018821992657401
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7501673197204416
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8238705491477792
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7797149988737974
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7577770420096137
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.8598155921155756
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.873701504685662
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7182045375758951
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7465290725231171
INFO:__main__:Finetuned loss: 0.5542344450950623
INFO:__main__:Meta Train Loss: 0.7245713458819822
INFO:__main__:Finetuned loss: 0.5542344450950623
Traceback (most recent call last):
  File "/zhome/2b/7/117471/Thesis/src/models/compare_metalearning.py", line 373, in <module>
    finetuned_transfer, optimizer_transfer = load_transfer_model(metamodel_abs_path, data_type=TYPE)
  File "/zhome/2b/7/117471/Thesis/src/models/compare_metalearning.py", line 164, in load_transfer_model
    edgeconv_transfer_state_dict = torch.load(f"{path}/finetuned_{data_type}_vanilla_model.pth", map_location=torch.device('cpu'))
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/torch/serialization.py", line 594, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/zhome/2b/7/117471/Thesis/venv-thesis/lib/python3.8/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/zhome/2b/7/117471/Thesis/metalearning/NOT-HOUR1/2021-10-20T11:26:27.443352/finetuned_REGION_vanilla_model.pth'
