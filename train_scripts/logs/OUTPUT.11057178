Starting:
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Epoch: 1
Meta Train Loss: 0.9836916923522949
########
Epoch: 2
Meta Train Loss: 0.8231989741325378
########
Epoch: 3
Meta Train Loss: 1.112734079360962
########
Epoch: 4
Meta Train Loss: 1.1400837898254395
########
Epoch: 5
Meta Train Loss: 0.9267168045043945
########
Epoch: 6
Meta Train Loss: 1.011666178703308
########
Epoch: 7
Meta Train Loss: 1.1392308473587036
########
Epoch: 8
Meta Train Loss: 1.1798657178878784
########
Epoch: 9
Meta Train Loss: 1.0745633840560913
########
Epoch: 10
Meta Train Loss: 1.0376557111740112
########
Epoch: 11
Meta Train Loss: 1.2193686962127686
########
Epoch: 12
Meta Train Loss: 1.0228132009506226
########
Epoch: 13
Meta Train Loss: 1.1711426973342896
########
Epoch: 14
Meta Train Loss: 1.1236772537231445
########
Epoch: 15
Meta Train Loss: 1.012337565422058
########
Epoch: 16
Meta Train Loss: 1.070931315422058
########
Epoch: 17
Meta Train Loss: 1.0645945072174072
########
Epoch: 18
Meta Train Loss: 0.9215595126152039
########
Epoch: 19
Meta Train Loss: 0.9331488013267517
########
Epoch: 20
Meta Train Loss: 1.0689516067504883
########
Epoch: 21
Meta Train Loss: 1.1224695444107056
########
Epoch: 22
Meta Train Loss: 1.1401276588439941
########
Epoch: 23
Meta Train Loss: 1.2102298736572266
########
Epoch: 24
Meta Train Loss: 0.9893917441368103
########
Epoch: 25
Meta Train Loss: 1.1054844856262207
########
Epoch: 26
Meta Train Loss: 1.1030558347702026
########
Epoch: 27
Meta Train Loss: 1.0108013153076172
########
Epoch: 28
Meta Train Loss: 1.0094341039657593
########
Epoch: 29
Meta Train Loss: 1.0070210695266724
########
Epoch: 30
Meta Train Loss: 0.9584338068962097
########
Epoch: 31
Meta Train Loss: 0.9191722273826599
########
Epoch: 32
Meta Train Loss: 1.0325580835342407
########
Epoch: 33
Meta Train Loss: 1.069812297821045
########
Epoch: 34
Meta Train Loss: 1.0999528169631958
########
Epoch: 35
Meta Train Loss: 0.9981064200401306
########
Epoch: 36
Meta Train Loss: 0.9306743741035461
########
Epoch: 37
Meta Train Loss: 1.0327476263046265
########
Epoch: 38
Meta Train Loss: 0.8168029189109802
########
Epoch: 39
Meta Train Loss: 0.885328471660614
########
Epoch: 40
Meta Train Loss: 1.1001372337341309
########
Epoch: 41
Meta Train Loss: 0.9809123873710632
########
Epoch: 42
Meta Train Loss: 1.0354334115982056
########
Epoch: 43
Meta Train Loss: 0.9667596817016602
########
Epoch: 44
Meta Train Loss: 0.9329184889793396
########
Epoch: 45
Meta Train Loss: 0.986625611782074
########
Epoch: 46
Meta Train Loss: 1.2977747917175293
########
Epoch: 47
Meta Train Loss: 0.9296890497207642
########
Epoch: 48
Meta Train Loss: 1.1393510103225708
########
Epoch: 49
Meta Train Loss: 1.0010489225387573
########
Epoch: 50
Meta Train Loss: 1.0252069234848022
########
Epoch: 51
Meta Train Loss: 0.9940760731697083
########
Epoch: 52
Meta Train Loss: 0.9725508093833923
########
Epoch: 53
Meta Train Loss: 1.0213274955749512
########
Epoch: 54
Meta Train Loss: 0.788252055644989
########
Epoch: 55
Meta Train Loss: 0.8782062530517578
########
Epoch: 56
Meta Train Loss: 0.7554281949996948
########
Epoch: 57
Meta Train Loss: 0.9675609469413757
########
Epoch: 58
Meta Train Loss: 0.9015774130821228
########
Epoch: 59
Meta Train Loss: 1.1533929109573364
########
Epoch: 60
Meta Train Loss: 0.9470762610435486
########
Epoch: 61
Meta Train Loss: 0.8113642930984497
########
Epoch: 62
Meta Train Loss: 1.0481373071670532
########
Epoch: 63
Meta Train Loss: 0.8230859637260437
########
Epoch: 64
Meta Train Loss: 0.9328886866569519
########
Epoch: 65
Meta Train Loss: 0.8688406944274902
########
Epoch: 66
Meta Train Loss: 0.9123145937919617
########
Epoch: 67
Meta Train Loss: 1.0331052541732788
########
Epoch: 68
Meta Train Loss: 0.8971865773200989
########
Epoch: 69
Meta Train Loss: 0.7593864798545837
########
Epoch: 70
Meta Train Loss: 0.8461489677429199
########
Epoch: 71
Meta Train Loss: 0.790104329586029
########
Epoch: 72
Meta Train Loss: 0.8371745944023132
########
Epoch: 73
Meta Train Loss: 1.0212595462799072
########
Epoch: 74
Meta Train Loss: 0.9685384035110474
########
Epoch: 75
Meta Train Loss: 0.9110963940620422
########
Epoch: 76
Meta Train Loss: 0.8308104872703552
########
Epoch: 77
Meta Train Loss: 0.8251751065254211
########
Epoch: 78
Meta Train Loss: 1.0800203084945679
########
Epoch: 79
Meta Train Loss: 0.7534753084182739
########
Epoch: 80
Meta Train Loss: 0.8609915971755981
########
Epoch: 81
Meta Train Loss: 0.7657340168952942
########
Epoch: 82
Meta Train Loss: 0.875111997127533
########
Epoch: 83
Meta Train Loss: 0.8973553776741028
########
Epoch: 84
Meta Train Loss: 0.9144817590713501
########
Epoch: 85
Meta Train Loss: 0.8764378428459167
########
Epoch: 86
Meta Train Loss: 0.9143908619880676
########
Epoch: 87
Meta Train Loss: 0.8339009284973145
########
Epoch: 88
Meta Train Loss: 0.7825616002082825
########
Epoch: 89
Meta Train Loss: 0.9490804672241211
########
Epoch: 90
Meta Train Loss: 0.7933743000030518
########
Epoch: 91
Meta Train Loss: 1.0239368677139282
########
Epoch: 92
Meta Train Loss: 0.894688606262207
########
Epoch: 93
Meta Train Loss: 1.021246075630188
########
Epoch: 94
Meta Train Loss: 0.7173309326171875
########
Epoch: 95
Meta Train Loss: 0.7476302981376648
########
Epoch: 96
Meta Train Loss: 0.8134280443191528
########
Epoch: 97
Meta Train Loss: 0.7731537222862244
########
Epoch: 98
Meta Train Loss: 0.803151547908783
########
Epoch: 99
Meta Train Loss: 0.9981415867805481
########
Epoch: 100
Meta Train Loss: 1.0012410879135132
########
Epoch: 101
Meta Train Loss: 0.7920366525650024
########
Epoch: 102
Meta Train Loss: 0.984313428401947
########
Epoch: 103
Meta Train Loss: 0.8435396552085876
########
Epoch: 104
Meta Train Loss: 1.072838306427002
########
Epoch: 105
Meta Train Loss: 0.8148048520088196
########
Epoch: 106
Meta Train Loss: 0.822727620601654
########
Epoch: 107
Meta Train Loss: 0.7004914879798889
########
Epoch: 108
Meta Train Loss: 0.8802562952041626
########
Epoch: 109
Meta Train Loss: 0.7918755412101746
########
Epoch: 110
Meta Train Loss: 0.9176515936851501
########
Epoch: 111
Meta Train Loss: 0.8627762198448181
########
Epoch: 112
Meta Train Loss: 0.9876853227615356
########
Epoch: 113
Meta Train Loss: 0.8853832483291626
########
Epoch: 114
Meta Train Loss: 0.7646565437316895
########
Epoch: 115
Meta Train Loss: 0.9507673382759094
########
Epoch: 116
Meta Train Loss: 0.8068267703056335
########
Epoch: 117
Meta Train Loss: 0.8986290097236633
########
Epoch: 118
Meta Train Loss: 0.9012663960456848
########
Epoch: 119
Meta Train Loss: 1.1276389360427856
########
Epoch: 120
Meta Train Loss: 0.6546425223350525
########
Epoch: 121
Meta Train Loss: 0.715628445148468
########
Epoch: 122
Meta Train Loss: 0.8999161720275879
########
Epoch: 123
Meta Train Loss: 1.135549783706665
########
Epoch: 124
Meta Train Loss: 0.9229182600975037
########
Epoch: 125
Meta Train Loss: 0.9502623677253723
########
Epoch: 126
Meta Train Loss: 0.8950651288032532
########
Epoch: 127
Meta Train Loss: 0.6510393023490906
########
Epoch: 128
Meta Train Loss: 0.7343282103538513
########
Epoch: 129
Meta Train Loss: 0.846138596534729
########
Epoch: 130
Meta Train Loss: 0.7959470748901367
########
Epoch: 131
Meta Train Loss: 0.8484140634536743
########
Epoch: 132
Meta Train Loss: 0.8995200395584106
########
Epoch: 133
Meta Train Loss: 0.7397243976593018
########
Epoch: 134
Meta Train Loss: 0.9487533569335938
########
Epoch: 135
Meta Train Loss: 0.7297959327697754
########
Epoch: 136
Meta Train Loss: 0.966070294380188
########
Epoch: 137
Meta Train Loss: 0.7652485966682434
########
Epoch: 138
Meta Train Loss: 0.9640766382217407
########
Epoch: 139
Meta Train Loss: 1.113179087638855
########
Epoch: 140
Meta Train Loss: 0.7693375945091248
########
Epoch: 141
Meta Train Loss: 0.6325885653495789
########
Epoch: 142
Meta Train Loss: 1.0424443483352661
########
Epoch: 143
Meta Train Loss: 0.873884379863739
########
Epoch: 144
Meta Train Loss: 0.72275310754776
########
Epoch: 145
Meta Train Loss: 0.9351045489311218
########
Epoch: 146
Meta Train Loss: 0.730500340461731
########
Epoch: 147
Meta Train Loss: 0.7779604196548462
########
Epoch: 148
Meta Train Loss: 0.8487440347671509
########
Epoch: 149
Meta Train Loss: 0.8686165809631348
########
Epoch: 150
Meta Train Loss: 0.8283696174621582
########
Epoch: 151
Meta Train Loss: 0.9742490649223328
########
Epoch: 152
Meta Train Loss: 0.9774093627929688
########
Epoch: 153
Meta Train Loss: 0.8283481597900391
########
Epoch: 154
Meta Train Loss: 0.6715901494026184
########
Epoch: 155
Meta Train Loss: 0.993229329586029
########
Epoch: 156
Meta Train Loss: 0.8535918593406677
########
Epoch: 157
Meta Train Loss: 0.8565278053283691
########
Epoch: 158
Meta Train Loss: 0.892457902431488
########
Epoch: 159
Meta Train Loss: 0.8554221987724304
########
Epoch: 160
Meta Train Loss: 1.2652587890625
########
Epoch: 161
Meta Train Loss: 0.927413284778595
########
Epoch: 162
Meta Train Loss: 0.9585430026054382
########
Epoch: 163
Meta Train Loss: 0.8972498774528503
########
Epoch: 164
Meta Train Loss: 0.9393424391746521
########
Epoch: 165
Meta Train Loss: 0.8695134520530701
########
Epoch: 166
Meta Train Loss: 0.813127338886261
########
Epoch: 167
Meta Train Loss: 0.7287883758544922
########
Epoch: 168
Meta Train Loss: 1.3382580280303955
########
Epoch: 169
Meta Train Loss: 0.8059342503547668
########
Epoch: 170
Meta Train Loss: 0.8108701705932617
########
Epoch: 171
Meta Train Loss: 0.8790479898452759
########
Epoch: 172
Meta Train Loss: 0.8428641557693481
########
Epoch: 173
Meta Train Loss: 0.7982386946678162
########
Epoch: 174
Meta Train Loss: 0.9435402154922485
########
Epoch: 175
Meta Train Loss: 0.7776055932044983
########
Epoch: 176
Meta Train Loss: 0.9537984728813171
########
Epoch: 177
Meta Train Loss: 0.890312135219574
########
Epoch: 178
Meta Train Loss: 0.8558128476142883
########
Epoch: 179
Meta Train Loss: 0.7747204303741455
########
Epoch: 180
Meta Train Loss: 0.823032021522522
########
Epoch: 181
Meta Train Loss: 0.7202037572860718
########
Epoch: 182
Meta Train Loss: 0.8804702758789062
########
Epoch: 183
Meta Train Loss: 0.8655946850776672
########
Epoch: 184
Meta Train Loss: 0.7253566384315491
########
Epoch: 185
Meta Train Loss: 0.8787146806716919
########
Epoch: 186
Meta Train Loss: 0.9518008232116699
########
Epoch: 187
Meta Train Loss: 0.7021862268447876
########
Epoch: 188
Meta Train Loss: 0.6016649603843689
########
Epoch: 189
Meta Train Loss: 0.726521909236908
########
Epoch: 190
Meta Train Loss: 0.8318930864334106
########
Epoch: 191
Meta Train Loss: 0.9363619685173035
########
Epoch: 192
Meta Train Loss: 0.8475615382194519
########
Epoch: 193
Meta Train Loss: 0.9388088583946228
########
Epoch: 194
Meta Train Loss: 0.8188849687576294
########
Epoch: 195
Meta Train Loss: 0.7075163722038269
########
Epoch: 196
Meta Train Loss: 0.7732927203178406
########
Epoch: 197
Meta Train Loss: 0.8495931029319763
########
Epoch: 198
Meta Train Loss: 0.8357699513435364
########
Epoch: 199
Meta Train Loss: 0.8555709719657898
########
Epoch: 200
Meta Train Loss: 0.9154877662658691
########
Epoch: 201
Meta Train Loss: 0.8893694281578064
########
Epoch: 202
Meta Train Loss: 0.8252040147781372
########
Epoch: 203
Meta Train Loss: 0.7161043882369995
########
Epoch: 204
Meta Train Loss: 0.7660616040229797
########
Epoch: 205
Meta Train Loss: 1.092663049697876
########
Epoch: 206
Meta Train Loss: 1.0477083921432495
########
Epoch: 207
Meta Train Loss: 0.9250026941299438
########
Epoch: 208
Meta Train Loss: 1.0736316442489624
########
Epoch: 209
Meta Train Loss: 0.8677830696105957
########
Epoch: 210
Meta Train Loss: 0.6919305324554443
########
Epoch: 211
Meta Train Loss: 0.9208526611328125
########
Epoch: 212
Meta Train Loss: 0.849377453327179
########
Epoch: 213
Meta Train Loss: 0.883546769618988
########
Epoch: 214
Meta Train Loss: 0.7126488089561462
########
Epoch: 215
Meta Train Loss: 0.8750889897346497
########
Epoch: 216
Meta Train Loss: 0.7875457406044006
########
Epoch: 217
Meta Train Loss: 0.9673811197280884
########
Epoch: 218
Meta Train Loss: 0.8194575309753418
########
Epoch: 219
Meta Train Loss: 0.8495697975158691
########
Epoch: 220
Meta Train Loss: 0.8862776160240173
########
Epoch: 221
Meta Train Loss: 0.8278452157974243
########
Epoch: 222
Meta Train Loss: 0.9248418211936951
########
Epoch: 223
Meta Train Loss: 0.774147629737854
########
Epoch: 224
Meta Train Loss: 0.8273473978042603
########
Epoch: 225
Meta Train Loss: 0.7457709312438965
########
Epoch: 226
Meta Train Loss: 0.7291144728660583
########
Epoch: 227
Meta Train Loss: 0.8077685236930847
########
Epoch: 228
Meta Train Loss: 0.9007337689399719
########
Epoch: 229
Meta Train Loss: 0.6784009337425232
########
Epoch: 230
Meta Train Loss: 0.8279646039009094
########
Epoch: 231
Meta Train Loss: 0.7541996836662292
########
Epoch: 232
Meta Train Loss: 0.781458854675293
########
Epoch: 233
Meta Train Loss: 0.7721563577651978
########
Epoch: 234
Meta Train Loss: 1.0178236961364746
########
Epoch: 235
Meta Train Loss: 0.7654622197151184
########
Epoch: 236
Meta Train Loss: 0.6527574062347412
########
Epoch: 237
Meta Train Loss: 0.8420745730400085
########
Epoch: 238
Meta Train Loss: 0.7858281135559082
########
Epoch: 239
Meta Train Loss: 1.0243431329727173
########
Epoch: 240
Meta Train Loss: 0.7828989028930664
########
Epoch: 241
Meta Train Loss: 0.9695164561271667
########
Epoch: 242
Meta Train Loss: 0.862464427947998
########
Epoch: 243
Meta Train Loss: 0.760483980178833
########
Epoch: 244
Meta Train Loss: 0.8826205134391785
########
Epoch: 245
Meta Train Loss: 1.1200364828109741
########
Epoch: 246
Meta Train Loss: 0.8899555206298828
########
Epoch: 247
Meta Train Loss: 0.7798265218734741
########
Epoch: 248
Meta Train Loss: 0.7307775616645813
########
Epoch: 249
Meta Train Loss: 0.9905287623405457
########
Epoch: 250
Meta Train Loss: 0.8662723898887634
########

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 11057178: <METALEARN> in cluster <dcc> Done

Job <METALEARN> was submitted from host <gbarlogin1> by user <tfehjo> in cluster <dcc> at Wed Nov 17 15:57:21 2021
Job was executed on host(s) <n-62-11-14>, in queue <gpuv100>, as user <tfehjo> in cluster <dcc> at Wed Nov 17 15:57:22 2021
</zhome/2b/7/117471> was used as the home directory.
</zhome/2b/7/117471/Thesis/train_scripts> was used as the working directory.
Started at Wed Nov 17 15:57:22 2021
Terminated at Wed Nov 17 17:29:08 2021
Results reported at Wed Nov 17 17:29:08 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
#BSUB -J METALEARN #The name the job will get
#BSUB -q gpuv100 #The queue the job will be committed to, here the GPU enabled queue
#BSUB -gpu "num=1:mode=exclusive_process" #How the job will be run on the VM, here I request 1 GPU with exclusive access i.e. only my c #BSUB -n 1 How many CPU cores my job request
#BSUB -W 24:00 #The maximum runtime my job have note that the queuing might enable shorter jobs earlier due to scheduling.
#BSUB -R "span[hosts=1]" #How many nodes the job requests
#BSUB -R "rusage[mem=12GB]" #How much RAM the job should have access to
#BSUB -R "select[gpu32gb]" #For requesting the extra big GPU w. 32GB of VRAM
#BSUB -o logs/OUTPUT.%J #Log file
#BSUB -e logs/ERROR.%J #Error log file
echo "Starting:"

cd ~/Thesis/metalearning
#cd /Users/theisferre/Documents/SPECIALE/Thesis/src/models

source ~/Thesis/venv-thesis/bin/activate

DATA_DIR=/zhome/2b/7/117471/Thesis/data/processed/ablation-augmented
TRAIN_SIZE=0.9
BATCH_TASK_SIZE=10
K_SHOT=5
ADAPTATION_STEPS=10
EPOCHS=250
ADAPT_LR=0.05
META_LR=0.001
EXCLUDE=citibike-tripdata,citibike2014,GM,green,LYFT,UBER
LOG_DIR=/zhome/2b/7/117471/Thesis/ablation-study/augmented
HIDDEN_SIZE=46
DROPOUT_P=0.2
NODE_OUT_FEATURES=10

# citibike-tripdata,citibike2014,GM,green,LYFT,TLC,UBER,yellow

python /zhome/2b/7/117471/Thesis/src/models/train_meta.py --data_dir $DATA_DIR --train_size $TRAIN_SIZE --batch_task_size $BATCH_TASK_SIZE \
--k_shot $K_SHOT --adaptation_steps $ADAPTATION_STEPS --epochs $EPOCHS --adapt_lr $ADAPT_LR --meta_lr $META_LR --log_dir $LOG_DIR \
--hidden_size $HIDDEN_SIZE --dropout_p $DROPOUT_P --node_out_features $NODE_OUT_FEATURES --exclude $EXCLUDE --gpu


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   5475.03 sec.
    Max Memory :                                 2589 MB
    Average Memory :                             2541.98 MB
    Total Requested Memory :                     12288.00 MB
    Delta Memory :                               9699.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   5506 sec.
    Turnaround time :                            5507 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/ERROR.11057178> for stderr output of this job.

