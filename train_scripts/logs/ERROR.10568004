INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/GM2017-july-sep-GRID.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.40947428887540643
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40990665555000305
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4060488695448095
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.410320214249871
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4099828451871872
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4079294367270036
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4114432470365004
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4072349491444501
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40992940014058893
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40942302346229553
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.41187240318818524
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4021017280491916
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4099715758453716
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40978961234742944
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.39899293942884967
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40634928237308154
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4006383581594987
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40167478268796747
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.41000628200444306
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40378504720601166
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4077756106853485
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.41407467831264844
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4113399169661782
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4063366123221137
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40673844380812213
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40929200703447516
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40178476680408826
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4148426516489549
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40918415920300916
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4084777398542924
INFO:__main__:Finetuned loss: 0.13630028678612274
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.40699138424613257
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3993385217406533
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4140686744993383
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40352628989653155
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3997650011019273
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40029684521935205
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4115420647642829
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4049835340543227
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.42967831817540253
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3982323272661729
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.38432594727386127
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3958632417700507
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.39086954431100324
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.39572037620977923
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.41452100060202857
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.29773219471628015
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.29495229639790277
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.36149889772588556
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3954070806503296
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4032328738407655
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40634793043136597
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4139695858413523
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3916826139796864
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.42801275849342346
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4052457538518039
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.43843037160960113
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4162376116622578
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4020944156429984
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.41195918348702515
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3892059624195099
INFO:__main__:Finetuned loss: 0.13630028678612274
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.3877477456222881
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.41184968839992175
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.372449438680302
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3971848745237697
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.37711317159912805
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.41781172156333923
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.43614210323853925
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3888539969921112
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4348977248777043
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.376665323972702
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.388216416944157
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.39041254737160425
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3854965730146928
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4134316959164359
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3968288844281977
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4037635854699395
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3361110280860554
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.36937622048638086
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.40001802281899884
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.46728430552916095
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4281392747705633
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3940568227659572
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3665914129127156
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4620843909003518
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.39923815564675763
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4073608788576993
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.45122631029649213
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.3989742005413229
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.4273859777233817
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:Meta Train Loss: 0.39614548601887445
INFO:__main__:Finetuned loss: 0.13630028678612274
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/yellow-taxi2020-nov-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8593462258577347
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8468456715345383
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8899029493331909
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8756137788295746
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8538765758275986
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8876406103372574
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8577313423156738
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8874935656785965
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8448765724897385
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8777630627155304
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8760572969913483
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8388747572898865
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8840401768684387
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.850361704826355
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8566674292087555
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8549660891294479
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8435465842485428
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8510145395994186
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8262959122657776
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.858353316783905
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8575428575277328
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8312327414751053
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8734510093927383
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8585634529590607
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8693135976791382
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8422867804765701
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8463504165410995
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8472780734300613
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8679540604352951
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.872251108288765
INFO:__main__:Finetuned loss: 0.8399074822664261
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8268207311630249
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8474031686782837
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.914210319519043
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8882326781749725
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8533700257539749
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8155217468738556
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.835132360458374
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8675971329212189
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8331174105405807
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8428093045949936
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8087443113327026
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.9229277968406677
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8531856536865234
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8235916048288345
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8492730557918549
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8555565029382706
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8672387152910233
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8478033691644669
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8834206461906433
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8278764337301254
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8801206201314926
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8578830510377884
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8794608265161514
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8799271136522293
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8209686130285263
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 1.4731884896755219
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8770304471254349
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8538385927677155
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.865693524479866
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8470276743173599
INFO:__main__:Finetuned loss: 0.8399074822664261
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.833576112985611
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8542096763849258
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8800996541976929
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8845960944890976
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.869238868355751
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8173451870679855
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8707290291786194
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8557754904031754
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8326046019792557
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8487730920314789
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8139340579509735
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8473796248435974
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8211181610822678
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8229525089263916
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8514248579740524
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8457692563533783
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.872926190495491
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8546139150857925
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8669461905956268
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8240581750869751
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8501033335924149
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8537223190069199
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8628075867891312
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8344910740852356
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8305117040872574
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8296805024147034
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8435707837343216
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8195239901542664
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8435932099819183
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:Meta Train Loss: 0.8526676446199417
INFO:__main__:Finetuned loss: 0.8399074822664261
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/LYFT2014-july-sep-GRID.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.1726438477635384
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1709680669009686
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1744225844740868
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1720581762492657
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.171196110546589
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1724995076656342
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1726817339658737
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1728311255574226
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1713888570666313
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1718799844384193
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1725832298398018
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1728180721402168
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.171928346157074
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1723077222704887
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.173412211239338
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1724420115351677
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1727079972624779
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1716285720467567
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1723910123109818
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1719771772623062
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1719669550657272
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.173083532601595
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1720971390604973
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1730041950941086
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1710136234760284
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1718593426048756
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1736147552728653
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1714405342936516
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1723733693361282
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1712247505784035
INFO:__main__:Finetuned loss: 1.166228998452425
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.1856173276901245
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1777181886136532
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1813139542937279
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1812761723995209
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1739629618823528
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1750401332974434
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.224265143275261
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1947201304137707
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1880903989076614
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1721129417419434
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1801500581204891
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1736962795257568
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1770414486527443
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 2.239048346877098
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1740849018096924
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.176456481218338
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.2909233272075653
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1766369119286537
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1721449196338654
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1750581189990044
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1739167794585228
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1782398074865341
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1891891583800316
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1940953582525253
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.2051601484417915
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1835414245724678
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1856503635644913
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1807191595435143
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.183535784482956
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1879750788211823
INFO:__main__:Finetuned loss: 1.166228998452425
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.1717224605381489
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1807940974831581
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1704234704375267
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1760792583227158
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.2208076864480972
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1865110099315643
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1981985047459602
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1768557094037533
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1788957864046097
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.184944935142994
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1757220774888992
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.172622799873352
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1727559678256512
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1685986556112766
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1701069176197052
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1862529069185257
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1830284297466278
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.189780406653881
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.178385317325592
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1735028773546219
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1865943297743797
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1985998190939426
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.175838552415371
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.18849216401577
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1751034371554852
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1870078034698963
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.178599514067173
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1923551671206951
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.1789570897817612
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:Meta Train Loss: 1.201235942542553
INFO:__main__:Finetuned loss: 1.166228998452425
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/UBER2015-jan-june-GRID.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7315449226986278
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7358433143659071
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.737978691404516
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.735464018854228
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7353096495975148
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.736759906465357
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7427851842208342
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7372129667888988
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7348414429209449
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.732759256254543
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.735447721047835
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7380555868148804
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7342342341488058
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.733894556760788
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7355413707819852
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.735134784470905
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7327684326605364
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7495622391050513
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7384088635444641
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7532459578730843
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7482865818522193
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7367548468438062
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7316275848583742
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7325697392225266
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7423763058402322
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7376812127503481
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7346725369041617
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7334820519794117
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.737020655111833
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7502200738950209
INFO:__main__:Finetuned loss: 0.70043484731154
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7449700723994862
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7353741377592087
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7392785969105634
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7420961978760633
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7405769621784036
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7417128384113312
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7455804646015167
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7378106848760084
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.748647846958854
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7598015124147589
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.758039502934976
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.736282164400274
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7736087441444397
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7614933956753124
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.780396664684469
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7385691539807753
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7371078838001598
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7495410482991826
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7582097785039381
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7459966919638894
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7549238502979279
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7435115806081078
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7387736168774691
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7355674654245377
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.73741115629673
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7298456796190955
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.734295901927081
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7365955358201807
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.740230227058584
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7385852336883545
INFO:__main__:Finetuned loss: 0.70043484731154
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7440746142105623
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7346919531171973
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7404832474210046
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7539860132065687
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.736531367356127
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7612580602819269
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7280095192519102
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7326624312184074
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.749147507277402
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7627207609740171
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7310886518521742
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7386793995445425
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7697776122526689
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7526017508723519
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7575213854963129
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.733111017129638
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7509903067892248
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7571309723637321
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7610443071885542
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.735308057882569
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7345502403649417
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7456215647133914
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.736505773934451
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.745534058321606
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7426387627016414
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7418064041571184
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7406850443644957
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7404800003225153
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7693912793289531
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:Meta Train Loss: 0.7698113240978934
INFO:__main__:Finetuned loss: 0.70043484731154
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/TLC2018-FHV-aug-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.4703798368573189
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.48236171156167984
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4637381136417389
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.47040998935699463
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4698563814163208
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.46864866465330124
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.47379568964242935
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.47330278903245926
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4655805379152298
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4723466783761978
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.477658174932003
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.47400836646556854
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4761864021420479
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4665817469358444
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4786888286471367
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4765273705124855
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.47705259919166565
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.48063185811042786
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4794095978140831
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4792650118470192
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4761013314127922
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.47897905111312866
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4668015018105507
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.46609362959861755
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.46628357470035553
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.48227589577436447
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.48130953311920166
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4800146594643593
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.47430893033742905
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.47280552238225937
INFO:__main__:Finetuned loss: 0.44089752435684204
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.48787232488393784
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.7328662425279617
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.42686478793621063
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5920767784118652
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.6737450808286667
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.7818358540534973
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5020042508840561
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.47357942163944244
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5522483065724373
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.6232227981090546
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5117093622684479
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.42585307359695435
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4205591604113579
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.43053845316171646
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5236585065722466
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5685643255710602
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.6294951811432838
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4544543996453285
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4340587481856346
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4969755634665489
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4538615196943283
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.48653141409158707
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4231409505009651
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.43890269100666046
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5816489011049271
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.6174925118684769
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.7869217246770859
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.44043590128421783
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4754432886838913
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4425961971282959
INFO:__main__:Finetuned loss: 0.44089752435684204
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.4318803623318672
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5152008980512619
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.42494672536849976
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.47319237142801285
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4773709550499916
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.571795716881752
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4605547711253166
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4274965450167656
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5059851035475731
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5443344041705132
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4188624992966652
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4721025824546814
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4676385223865509
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.48114627599716187
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.43151959776878357
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5653160437941551
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4479392468929291
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5783573091030121
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4183647409081459
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.45066770166158676
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5250984355807304
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.520073652267456
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5468339323997498
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.4231150150299072
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.44011257588863373
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5225991830229759
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.5887952223420143
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.41606488078832626
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.42231954634189606
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:Meta Train Loss: 0.44945744425058365
INFO:__main__:Finetuned loss: 0.44089752435684204
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/T-Drive-taxi-pickups-GRID.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.1169676780700684
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1156952381134033
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.118924856185913
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.115304708480835
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.124725341796875
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.113210916519165
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1162333488464355
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1164262294769287
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.11655855178833
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1149742603302002
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1150516271591187
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.11661696434021
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.116626501083374
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.122076392173767
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1176319122314453
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.116989016532898
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1129701137542725
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1191351413726807
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1161233186721802
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1188013553619385
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1195058822631836
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.119346261024475
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.115131139755249
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.112917423248291
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1133074760437012
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1184321641921997
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.116835117340088
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.119102120399475
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1181548833847046
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1164919137954712
INFO:__main__:Finetuned loss: 1.0889883041381836
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.1039015054702759
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1116958856582642
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.091640830039978
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1603927612304688
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1387660503387451
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1072266101837158
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0990312099456787
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.090575098991394
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0936404466629028
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1167936325073242
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.126880168914795
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0899815559387207
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1497994661331177
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1476104259490967
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0978996753692627
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1152228116989136
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1055867671966553
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0755066871643066
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1169848442077637
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1202183961868286
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0988097190856934
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1057443618774414
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0996366739273071
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1104388236999512
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.106502890586853
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0741342306137085
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1044843196868896
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1163243055343628
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.081981897354126
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0858607292175293
INFO:__main__:Finetuned loss: 1.0889883041381836
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.0952945947647095
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0909897089004517
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0888936519622803
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0860612392425537
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0854028463363647
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0893522500991821
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0950599908828735
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.095427393913269
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1003714799880981
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0970470905303955
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0978972911834717
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0864571332931519
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1095757484436035
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1238903999328613
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0826950073242188
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0936620235443115
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.084381103515625
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0856231451034546
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0853338241577148
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.079679250717163
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1087850332260132
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0849461555480957
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.087742567062378
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0948545932769775
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.1114954948425293
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0863871574401855
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.083304524421692
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0832384824752808
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.092181921005249
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:Meta Train Loss: 1.0965114831924438
INFO:__main__:Finetuned loss: 1.0889883041381836
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/GM2017-july-sep-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.0773940411480991
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0769611651247197
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0775217413902283
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0774299448186702
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0774031552401455
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.077443317933516
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0779452378099614
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0769123272462324
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0773554335940967
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0783102729103782
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0773323069919238
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0775440768762068
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.078272440216758
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0783067508177324
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0775964856147766
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0772810686718335
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0779705155979504
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.078508821400729
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0773139704357495
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.077345457944003
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0781354795802722
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0779062997211108
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.076849260113456
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0775079131126404
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0784773718227039
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.077919358556921
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0773207274350254
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.077263057231903
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0783753503452649
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.077796226198023
INFO:__main__:Finetuned loss: 1.024370713667436
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.0806761167266152
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.058021529154344
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.047667839310386
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0526550087061795
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0724493590268223
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0655062740499324
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0468934286724438
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0457049012184143
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0615270679647273
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0503874529491772
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0575677373192527
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0551552447405728
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0525880401784724
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0650029290806164
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.07187189839103
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.1076629216020757
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0986306396397678
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0562189817428589
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.072374333034862
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0562560558319092
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.071289149197665
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.051293513991616
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0709066499363293
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.04838088425723
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.068319943818179
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0715621601451526
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0595511577346108
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0501434206962585
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.051613206213171
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0684245987371965
INFO:__main__:Finetuned loss: 1.024370713667436
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 1.0722350153056057
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0559337735176086
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.052470158446919
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.05242689631202
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.052053158933466
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0624117959629407
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0549998066642068
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0497076836499302
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0639684362844988
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0532229596918279
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0587561347267844
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0545364835045554
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0531884919513355
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0696277022361755
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0594192038882861
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0726440332152627
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0721196423877368
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0466791283000598
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.05258400331844
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.057227156379006
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0515216914090244
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0520083037289707
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0606287663633174
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0566543557427146
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0578979145396838
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0584630641070278
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0629099932583896
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.048564688725905
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0593264265493914
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:Meta Train Loss: 1.0638127706267617
INFO:__main__:Finetuned loss: 1.024370713667436
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/UBER2015-jan-june-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7500469494949688
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7533711222085085
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7535937536846508
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.752405586567792
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.75349914485758
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7528781430287794
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7531139335849069
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7520045421340249
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7514112889766693
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7501789209517565
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7523184825073589
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7531756514852698
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7525017207319086
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7518759532408281
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7521360150792382
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.750773318789222
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7516369386152788
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7557585483247583
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7558794482187792
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7591118351979689
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7539084458892996
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7549805370244113
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7492297941988165
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7516740804368799
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7581294666637074
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7525645711205222
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7515789351680062
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7504500259052623
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7523178647864949
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7572135844013907
INFO:__main__:Finetuned loss: 0.7093578170646321
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7703877904198386
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7566413608464327
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7541709569367495
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7550276436588981
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7576030221852389
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.750481058250774
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7624509578401392
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7804292088205164
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7547122538089752
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7625639926303517
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.782064364715056
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7505058483643965
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7523037357763811
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7709939967502247
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7564168193123557
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7678844115950845
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7564409835772081
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7636403197591956
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7583188089457426
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7536671757698059
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7740753184665333
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7526453625072133
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7722960249944166
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7438015748154033
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.743943149393255
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7510056441480463
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7490923160856421
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7462314556945454
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7581327936866067
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.746615545316176
INFO:__main__:Finetuned loss: 0.7093578170646321
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7744667828083038
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7471257177266207
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7616318328814073
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7568315159190785
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7528814321214502
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7722032043066892
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7494859478690408
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7486673295497894
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7538853910836306
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7602523240176114
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7530931234359741
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7569629197770898
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7509476407007738
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7731458598917181
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7599053355780515
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7540543323213403
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7691327197985216
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.751193415034901
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7533200085163116
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7468777813694694
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7612779303030535
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7576300393451344
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7512009929526936
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7629011855884031
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7490464069626548
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7526771642945029
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7519806975668127
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7483488294211301
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.79521136934107
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:Meta Train Loss: 0.7794189182194796
INFO:__main__:Finetuned loss: 0.7093578170646321
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/green-taxi2020-dec-GRID.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7118387296795845
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7330894321203232
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7241137772798538
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7333622500300407
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7364325523376465
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7376106083393097
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7442578226327896
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7481433600187302
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7096008509397507
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7250012308359146
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7329490929841995
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.731221467256546
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7403459548950195
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7371390461921692
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7257138788700104
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7256161645054817
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7374569624662399
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7291949838399887
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7256942689418793
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7312778383493423
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.72012659907341
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7327948063611984
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7338955402374268
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7258465439081192
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7368989884853363
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7166204452514648
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7557527273893356
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7829702794551849
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7423002123832703
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7213946133852005
INFO:__main__:Finetuned loss: 0.6961377039551735
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7237809598445892
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7421897873282433
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7292390093207359
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7606966495513916
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.720084011554718
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7236795276403427
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7230179756879807
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7388382107019424
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7254434674978256
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7278047353029251
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7049803733825684
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7457541972398758
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7081909328699112
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7883748486638069
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7242280542850494
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7373131811618805
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7046130076050758
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7381317764520645
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 958.2974739074707
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7542230933904648
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7002123594284058
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7249354720115662
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7432200312614441
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7300763428211212
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7284720242023468
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7704125866293907
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7122896984219551
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7405078411102295
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7347524464130402
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7446143925189972
INFO:__main__:Finetuned loss: 0.6961377039551735
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7053659856319427
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.698322668671608
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7009160816669464
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7420798540115356
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7115787267684937
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7284776568412781
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7233379781246185
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.721995085477829
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7042032778263092
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7187792509794235
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.6989354938268661
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.6968394815921783
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.6874038726091385
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7635091096162796
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.683660127222538
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7429488003253937
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.6958235651254654
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.71846804022789
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7175610810518265
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7177804112434387
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7078286409378052
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7055655866861343
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7054882049560547
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.716092973947525
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7206924706697464
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7130065783858299
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7291776016354561
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7214628756046295
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7081239968538284
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:Meta Train Loss: 0.7126693725585938
INFO:__main__:Finetuned loss: 0.6961377039551735
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/TLC2018-FHV-aug-GRID.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.430887907743454
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43771328032016754
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.42879873514175415
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43329183012247086
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4314502477645874
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.42842748016119003
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4331653043627739
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4289693534374237
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43134016543626785
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.42686905711889267
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43794936686754227
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4399239793419838
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4295060783624649
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4275287240743637
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43906913697719574
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4503373056650162
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.44122960418462753
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.44978251308202744
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4404417872428894
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43450137972831726
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43858782947063446
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4428209513425827
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.42330634593963623
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43398940563201904
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4326558858156204
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43788379430770874
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.44239169359207153
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.42951857298612595
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4364183843135834
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4336492568254471
INFO:__main__:Finetuned loss: 0.4251868203282356
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.47035984694957733
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.6729915589094162
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.41018518060445786
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5551982522010803
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.613663524389267
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.7611304968595505
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.48324909806251526
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4875394403934479
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5239415913820267
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.6022269874811172
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.46163250505924225
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.41360612213611603
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4141080379486084
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.40582989156246185
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.48348578065633774
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5489671230316162
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.6223042011260986
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4274713844060898
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.44010937213897705
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4967377036809921
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4382464066147804
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4865461438894272
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.39857738465070724
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4369701221585274
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5539857447147369
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5639267340302467
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.6912386938929558
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4233591556549072
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.45959629863500595
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.412188857793808
INFO:__main__:Finetuned loss: 0.4251868203282356
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.4256347343325615
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5046940892934799
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.40626955032348633
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43666546046733856
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4293675497174263
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5506474077701569
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4366818368434906
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4310731813311577
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4846131280064583
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5077652409672737
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.40971385687589645
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4365837201476097
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.44572873413562775
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.444392554461956
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.41153180599212646
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5139434337615967
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.43152856081724167
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5368102937936783
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.40783897787332535
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4327271208167076
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.486331470310688
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.518017552793026
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5003755018115044
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.42215460538864136
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.42836733162403107
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.48678847402334213
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.5428712964057922
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.4094284400343895
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.41616756469011307
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:Meta Train Loss: 0.41898082196712494
INFO:__main__:Finetuned loss: 0.4251868203282356
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/citibike-tripdata-GRID.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.41808112913911993
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41662008383057336
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4180501672354611
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4170019816268574
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4179571812803095
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4164950576695529
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4173551391471516
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4179145666685971
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41636012359098956
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41942042925141076
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41828280145471747
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41770208694718103
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4184509976343675
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4177288683977994
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41638409007679333
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4176856739954515
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4163553606380116
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4153366332704371
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41853164542805066
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41681295226920734
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41826330802657385
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41934287548065186
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4193121953444047
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41618249362165277
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4188472195105119
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41758621551773767
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4186146692796187
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41642404957251117
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4172734970396215
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41892298785122956
INFO:__main__:Finetuned loss: 0.37373731623996387
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.40480199456214905
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.5070172358642925
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.40572384812615137
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4099320660937916
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.466588323766535
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.5017017098990354
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.8211277506568215
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.3681147071448239
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.47054427049376746
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4078375846147537
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.36922416903755884
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4217573783614419
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.5034208189357411
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4079666191881353
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.43744354898279364
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.37188195369460364
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.38754490559751337
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4953259392218156
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4113205779682506
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.38598969849673187
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.46263958378271625
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.3698084462772716
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4451922530477697
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.6311322071335532
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 40158.79905007102
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4849088273265145
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.40436526320197363
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4358582713387229
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.45556107976219873
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.37498526681553235
INFO:__main__:Finetuned loss: 0.37373731623996387
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.41140285405245697
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.42022273486310785
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.36740879579023883
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.43104908954013477
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.40238159082152625
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.44024656035683374
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.3819779266010631
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41044430570168927
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4085746840997176
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.38292874666777527
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.611582181670449
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4125204546885057
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4560903419147838
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.37550294399261475
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4541496119715951
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.41069404645399615
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.372354504736987
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.36697927388277923
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.38579505681991577
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.38276309316808527
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.39202987334944983
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.3829310617663644
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.37475554780526593
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.574730864980004
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.3744767091491006
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4580791375853799
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4499169940298254
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.4054788920012387
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.3819196332584728
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:Meta Train Loss: 0.3558608510277488
INFO:__main__:Finetuned loss: 0.37373731623996387
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/yellow-taxi2020-nov-GRID.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.8145574629306793
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7627575993537903
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8209220618009567
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7749339640140533
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7683150619268417
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8038787096738815
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7817470729351044
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8615740835666656
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7650384902954102
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8570476621389389
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7798111885786057
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7774453908205032
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7815335839986801
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7572586089372635
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7787468135356903
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7789060771465302
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7586024254560471
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7882517576217651
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7486725151538849
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7906935662031174
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7898864597082138
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7495542615652084
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7873626798391342
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7914006859064102
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7754539549350739
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7659642398357391
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7611130774021149
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7678067833185196
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8084513694047928
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.787766233086586
INFO:__main__:Finetuned loss: 0.7461683452129364
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7281840592622757
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7413477152585983
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7883000075817108
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8361120745539665
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8325360268354416
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7774357795715332
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7346796840429306
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7576721459627151
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.760945126414299
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7828862369060516
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7195416986942291
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.788787230849266
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7655699402093887
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7322696298360825
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7542661726474762
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7663891613483429
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7751824259757996
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7588125765323639
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7674069702625275
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8354160636663437
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8654760271310806
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7606310397386551
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7765803039073944
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.766937643289566
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7501941621303558
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 1.451416864991188
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7507369071245193
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7326857671141624
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7868743538856506
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7729617953300476
INFO:__main__:Finetuned loss: 0.7461683452129364
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7334305197000504
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.747041329741478
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7999514192342758
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8549401313066483
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8395308703184128
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7453649044036865
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7516529262065887
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.764460876584053
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7638653665781021
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.756460964679718
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.715559646487236
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7615157663822174
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7338404655456543
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7337048649787903
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7529994696378708
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7556187063455582
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7864404320716858
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7734483480453491
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7498325854539871
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8045752495527267
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.8004216253757477
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7563385516405106
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7488333433866501
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7356572151184082
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7507994621992111
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.757423996925354
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7452022582292557
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7253502681851387
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.7575542330741882
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:Meta Train Loss: 0.779029980301857
INFO:__main__:Finetuned loss: 0.7461683452129364
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/green-taxi2020-dec-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.9365581125020981
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9517723768949509
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9635351896286011
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9583269208669662
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9715272039175034
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9886337369680405
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9810391217470169
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9658059775829315
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9414583146572113
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9541139155626297
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9558065980672836
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9639417976140976
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9630635976791382
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9647886902093887
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9493263214826584
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9504873901605606
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9572966545820236
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.969207763671875
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.950114980340004
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9443527460098267
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9510519802570343
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9665883928537369
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9745735824108124
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9635387510061264
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9643259048461914
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9427602738142014
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9755389541387558
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 1.009840413928032
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9704803377389908
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9642315208911896
INFO:__main__:Finetuned loss: 0.9386760741472244
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.9353977143764496
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9607534259557724
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9708269089460373
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9709355384111404
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9536558091640472
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9613445997238159
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9507228136062622
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9340147227048874
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9631392359733582
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9443245530128479
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9626152813434601
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9451404511928558
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9323995560407639
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 1.010355293750763
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9675827175378799
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9645680040121078
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9363820254802704
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9692718684673309
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 2968.2002563476562
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9680414646863937
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9451138526201248
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9489253163337708
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9628865420818329
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.933023989200592
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.964250460267067
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9831109344959259
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9357160776853561
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9526360332965851
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9868882447481155
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9525469839572906
INFO:__main__:Finetuned loss: 0.9386760741472244
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.9259966611862183
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9354838728904724
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9602417349815369
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9622778296470642
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9383220970630646
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9590451568365097
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9510938376188278
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9393059611320496
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9530760645866394
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9327811598777771
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9437622576951981
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9411347210407257
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9196607768535614
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9900182783603668
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9387906789779663
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9653434157371521
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9372942894697189
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9576268047094345
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9311634749174118
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9659261107444763
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9331165850162506
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9371238052845001
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.947714775800705
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9360550940036774
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.953453853726387
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9562503695487976
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9280874133110046
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9526600688695908
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.970984622836113
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:Meta Train Loss: 0.9247852116823196
INFO:__main__:Finetuned loss: 0.9386760741472244
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/citibike2014-tripdata-GRID.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.5028614483096383
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5034326680681922
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5033497972921892
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5031060454520312
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5050483793020248
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5035024962641976
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.504082056609067
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.504916640845212
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5033624477007173
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5049156126650897
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5021968429738825
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5036207762631503
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5019445392218503
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5045365250923417
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5054262808778069
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5053033889694647
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5034795281561938
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5062655996192585
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5060917376117273
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5028415281664241
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.505267858505249
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5028784823688593
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5024374642155387
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.505545321513306
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5049344389276071
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5057327246124094
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5046985162930056
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5050671303814108
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5046404532410882
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5049457238479094
INFO:__main__:Finetuned loss: 0.4826359206979925
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.559360998598012
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 1.8934396288611672
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5041190819306807
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5101172761483626
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5183508653532375
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.6714585233818401
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5050263066183437
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5329302583228458
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.51105074449019
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5078691068020734
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5019236769188534
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5963124057108705
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5012654309922998
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5094632099975239
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5626071285117756
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.9719051745804873
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5155032520944421
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5575597150759264
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.542099307206544
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5472633201967586
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.8451510505242781
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.4973448393019763
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.52106389132413
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5316975617950613
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5628191192041744
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5653378191319379
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5393294312737205
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.502943525260145
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5130858333273367
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5011719221418555
INFO:__main__:Finetuned loss: 0.4826359206979925
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.5323933281681754
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.522418194196441
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5252957438880747
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5087813667275689
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5087935545227744
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5739585581150922
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5131945975802161
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5313198823820461
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5012436969713732
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.514168613336303
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5319481512362306
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5784864310513843
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5097538205710325
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5005702444098212
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5596339743245732
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5794454934922132
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5582242702895944
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5238400101661682
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5195489214225248
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5650984644889832
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5839843831279061
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5126291743733666
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5469753850590099
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5159731696952473
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5597965459931981
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5086150589314374
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5690496753562581
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5135796611959283
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5990149243311449
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:Meta Train Loss: 0.5035768246108835
INFO:__main__:Finetuned loss: 0.4826359206979925
INFO:__main__:/zhome/2b/7/117471/Thesis/data/processed/metalearning/citibike2014-tripdata-REGION.pkl
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.6866237209601835
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6877974339506843
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6873130391944539
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6867747631939974
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6887465823780407
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.686965755440972
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6883907169103622
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6886751082810488
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6873055967417631
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6878465075384487
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6870967840606516
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6885275488550012
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6874769262292169
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6896770420399579
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.687938155098395
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6886337033726952
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6871160932562568
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6879690587520599
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6888963078910654
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6868401887741956
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.68782493065704
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6866682347926226
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6881786842237819
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6881268606944517
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.688256638971242
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.688968455249613
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6881048679351807
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.687545895576477
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6881301159208472
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6876086416569623
INFO:__main__:Finetuned loss: 0.6724194559183988
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7190558138218793
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 1.4510024650530382
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6811826418746602
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.692960420792753
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6960337256843393
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7676834531805732
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6843083649873734
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6875035898251967
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6900441484017805
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6867633936080065
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6806608316573229
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7406492328101938
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6793361495841633
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6851836199110205
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7220430319959467
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.9337317049503326
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6865010952407663
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7039302275939421
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7169267318465493
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7094027142633091
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.8340741341764276
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6793329119682312
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6953304613178427
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7131500081582502
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7064128694209185
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7362591949376193
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7042925913225521
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6818121454932473
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6860348202965476
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6795633611353961
INFO:__main__:Finetuned loss: 0.6724194559183988
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
INFO:__main__:Meta Train Loss: 0.7146208448843523
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.694695847955617
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6947805149988695
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6845199845053933
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6828651617873799
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.727441594004631
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6870263693007556
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6910282983021303
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6801818406040018
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6836743314157833
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6928093446926638
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7341013699769974
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6884342594580217
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6775229397145185
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.71949918161739
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.711173025044528
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.725206583738327
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6950971673835408
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7015893093564294
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7058562771840529
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7499159222299402
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6876653324473988
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.711028509519317
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6802813072096218
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7122554833238776
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6849753463810141
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7102200348268856
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6820410909977827
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.7207347303628922
INFO:__main__:Finetuned loss: 0.6724194559183988
INFO:__main__:Meta Train Loss: 0.6852934604341333
INFO:__main__:Finetuned loss: 0.6724194559183988
