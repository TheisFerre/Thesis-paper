Starting:
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Shuffling data...
Epoch: 1
Meta Train Loss: 0.8632708787918091
########
Epoch: 2
Meta Train Loss: 1.2547849416732788
########
Epoch: 3
Meta Train Loss: 0.8883419036865234
########
Epoch: 4
Meta Train Loss: 1.2433874607086182
########
Epoch: 5
Meta Train Loss: 0.8672658801078796
########
Epoch: 6
Meta Train Loss: 0.9944812655448914
########
Epoch: 7
Meta Train Loss: 0.9712695479393005
########
Epoch: 8
Meta Train Loss: 0.8275181651115417
########
Epoch: 9
Meta Train Loss: 196.80393981933594
########
Epoch: 10
Meta Train Loss: 0.793524444103241
########
Epoch: 11
Meta Train Loss: 1.5069198608398438
########
Epoch: 12
Meta Train Loss: 0.8924530148506165
########
Epoch: 13
Meta Train Loss: 1.5444509983062744
########
Epoch: 14
Meta Train Loss: 1.0950927734375
########
Epoch: 15
Meta Train Loss: 1.511438250541687
########
Epoch: 16
Meta Train Loss: 0.863357663154602
########
Epoch: 17
Meta Train Loss: 1.0401580333709717
########
Epoch: 18
Meta Train Loss: 1.1777235269546509
########
Epoch: 19
Meta Train Loss: 0.7976638078689575
########
Epoch: 20
Meta Train Loss: 0.861674964427948
########
Epoch: 21
Meta Train Loss: 0.9711760878562927
########
Epoch: 22
Meta Train Loss: 1.308044672012329
########
Epoch: 23
Meta Train Loss: 1.2786692380905151
########
Epoch: 24
Meta Train Loss: 0.8096570372581482
########
Epoch: 25
Meta Train Loss: 1.0471556186676025
########
Epoch: 26
Meta Train Loss: 15.375805854797363
########
Epoch: 27
Meta Train Loss: 1.1795885562896729
########
Epoch: 28
Meta Train Loss: 0.9155740141868591
########
Epoch: 29
Meta Train Loss: 16299226.0
########
Epoch: 30
Meta Train Loss: 0.7731567621231079
########
Epoch: 31
Meta Train Loss: 0.8429989218711853
########
Epoch: 32
Meta Train Loss: 1.3099483251571655
########
Epoch: 33
Meta Train Loss: 0.9320335388183594
########
Epoch: 34
Meta Train Loss: 0.9299927949905396
########
Epoch: 35
Meta Train Loss: 0.8160997629165649
########
Epoch: 36
Meta Train Loss: 1.17184579372406
########
Epoch: 37
Meta Train Loss: 0.7216815948486328
########
Epoch: 38
Meta Train Loss: 1.2084563970565796
########
Epoch: 39
Meta Train Loss: 1.058458685874939
########
Epoch: 40
Meta Train Loss: 1.221643328666687
########
Epoch: 41
Meta Train Loss: 0.8430700302124023
########
Epoch: 42
Meta Train Loss: 0.8664270639419556
########
Epoch: 43
Meta Train Loss: 0.7952231764793396
########
Epoch: 44
Meta Train Loss: 0.7428532242774963
########
Epoch: 45
Meta Train Loss: 0.8130761981010437
########
Epoch: 46
Meta Train Loss: 0.8666691780090332
########
Epoch: 47
Meta Train Loss: 0.655522346496582
########
Epoch: 48
Meta Train Loss: 0.6185263991355896
########
Epoch: 49
Meta Train Loss: 0.7111518383026123
########
Epoch: 50
Meta Train Loss: 0.8529826998710632
########
Epoch: 51
Meta Train Loss: 0.6778209805488586
########
Epoch: 52
Meta Train Loss: 0.8442985415458679
########
Epoch: 53
Meta Train Loss: 1.3946138620376587
########
Epoch: 54
Meta Train Loss: 0.9428300857543945
########
Epoch: 55
Meta Train Loss: 0.787712037563324
########
Epoch: 56
Meta Train Loss: 0.7294256091117859
########
Epoch: 57
Meta Train Loss: 0.7742549777030945
########
Epoch: 58
Meta Train Loss: 0.8168122172355652
########
Epoch: 59
Meta Train Loss: 0.6602544784545898
########
Epoch: 60
Meta Train Loss: 1.04615318775177
########
Epoch: 61
Meta Train Loss: 0.7541128396987915
########
Epoch: 62
Meta Train Loss: 0.5402864813804626
########
Epoch: 63
Meta Train Loss: 0.7015137076377869
########
Epoch: 64
Meta Train Loss: 96300392448.0
########
Epoch: 65
Meta Train Loss: 0.8359144330024719
########
Epoch: 66
Meta Train Loss: 0.6961543560028076
########
Epoch: 67
Meta Train Loss: 0.7256473898887634
########
Epoch: 68
Meta Train Loss: 0.6369392275810242
########
Epoch: 69
Meta Train Loss: 0.5588249564170837
########
Epoch: 70
Meta Train Loss: 0.7494350671768188
########
Epoch: 71
Meta Train Loss: 1.1374403238296509
########
Epoch: 72
Meta Train Loss: 0.8034944534301758
########
Epoch: 73
Meta Train Loss: 0.7189540863037109
########
Epoch: 74
Meta Train Loss: 0.7095473408699036
########
Epoch: 75
Meta Train Loss: 0.917707085609436
########
Epoch: 76
Meta Train Loss: 0.6852656602859497
########
Epoch: 77
Meta Train Loss: 0.6969464421272278
########
Epoch: 78
Meta Train Loss: 0.7582696080207825
########
Epoch: 79
Meta Train Loss: 0.6544058918952942
########
Epoch: 80
Meta Train Loss: 0.651222825050354
########
Epoch: 81
Meta Train Loss: 0.7221061587333679
########
Epoch: 82
Meta Train Loss: 10.938265800476074
########
Epoch: 83
Meta Train Loss: 0.8138338923454285
########
Epoch: 84
Meta Train Loss: 0.6490811109542847
########
Epoch: 85
Meta Train Loss: 0.8519951701164246
########
Epoch: 86
Meta Train Loss: 0.7893713116645813
########
Epoch: 87
Meta Train Loss: 0.7186281681060791
########
Epoch: 88
Meta Train Loss: 0.828748345375061
########
Epoch: 89
Meta Train Loss: 0.9301916360855103
########
Epoch: 90
Meta Train Loss: 0.7108154892921448
########
Epoch: 91
Meta Train Loss: 0.6510884761810303
########
Epoch: 92
Meta Train Loss: 1.028326153755188
########
Epoch: 93
Meta Train Loss: 0.7480964660644531
########
Epoch: 94
Meta Train Loss: 0.5093002319335938
########
Epoch: 95
Meta Train Loss: 134672176.0
########
Epoch: 96
Meta Train Loss: 0.6877254843711853
########
Epoch: 97
Meta Train Loss: 0.6615564227104187
########
Epoch: 98
Meta Train Loss: 1.0340471267700195
########
Epoch: 99
Meta Train Loss: 0.7415323853492737
########
Epoch: 100
Meta Train Loss: 0.6870861649513245
########
Epoch: 101
Meta Train Loss: 0.7059981226921082
########
Epoch: 102
Meta Train Loss: 0.9098140001296997
########
Epoch: 103
Meta Train Loss: 0.6817174553871155
########
Epoch: 104
Meta Train Loss: 0.6313275694847107
########
Epoch: 105
Meta Train Loss: 0.7735124826431274
########
Epoch: 106
Meta Train Loss: 0.6181137561798096
########
Epoch: 107
Meta Train Loss: 0.6347483396530151
########
Epoch: 108
Meta Train Loss: 0.6527435183525085
########
Epoch: 109
Meta Train Loss: 0.7186998724937439
########
Epoch: 110
Meta Train Loss: 7081417.0
########
Epoch: 111
Meta Train Loss: 12160.0361328125
########
Epoch: 112
Meta Train Loss: 0.8699097037315369
########
Epoch: 113
Meta Train Loss: 0.6161465048789978
########
Epoch: 114
Meta Train Loss: 0.6763284206390381
########
Epoch: 115
Meta Train Loss: 0.6005024909973145
########
Epoch: 116
Meta Train Loss: 0.8897948265075684
########
Epoch: 117
Meta Train Loss: 2.999460220336914
########
Epoch: 118
Meta Train Loss: 0.7043856382369995
########
Epoch: 119
Meta Train Loss: 0.8510899543762207
########
Epoch: 120
Meta Train Loss: 0.9187664985656738
########
Epoch: 121
Meta Train Loss: 0.8096948862075806
########
Epoch: 122
Meta Train Loss: 0.8057205080986023
########
Epoch: 123
Meta Train Loss: 0.8146229982376099
########
Epoch: 124
Meta Train Loss: 0.7005089521408081
########
Epoch: 125
Meta Train Loss: 23.277196884155273
########
Epoch: 126
Meta Train Loss: 0.772988498210907
########
Epoch: 127
Meta Train Loss: 0.6906425356864929
########
Epoch: 128
Meta Train Loss: 0.6488718390464783
########
Epoch: 129
Meta Train Loss: 0.9092164039611816
########
Epoch: 130
Meta Train Loss: 0.8035926818847656
########
Epoch: 131
Meta Train Loss: 1.2620793581008911
########
Epoch: 132
Meta Train Loss: 0.7191663980484009
########
Epoch: 133
Meta Train Loss: 18.51938819885254
########
Epoch: 134
Meta Train Loss: 0.7496632933616638
########
Epoch: 135
Meta Train Loss: 0.7402986884117126
########
Epoch: 136
Meta Train Loss: 0.6689454913139343
########
Epoch: 137
Meta Train Loss: 0.8493064045906067
########
Epoch: 138
Meta Train Loss: 0.6548057794570923
########
Epoch: 139
Meta Train Loss: 1.2169501781463623
########
Epoch: 140
Meta Train Loss: 0.6224471926689148
########
Epoch: 141
Meta Train Loss: 0.5966251492500305
########
Epoch: 142
Meta Train Loss: 0.7393639087677002
########
Epoch: 143
Meta Train Loss: 0.750089168548584
########
Epoch: 144
Meta Train Loss: 0.665045440196991
########
Epoch: 145
Meta Train Loss: 0.7392081618309021
########
Epoch: 146
Meta Train Loss: 0.6418291926383972
########
Epoch: 147
Meta Train Loss: 0.7357209920883179
########
Epoch: 148
Meta Train Loss: 0.8121582865715027
########
Epoch: 149
Meta Train Loss: 0.831771194934845
########
Epoch: 150
Meta Train Loss: 0.6348286867141724
########

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 11148317: <METALEARN> in cluster <dcc> Done

Job <METALEARN> was submitted from host <gbarlogin1> by user <tfehjo> in cluster <dcc> at Wed Nov 24 15:31:36 2021
Job was executed on host(s) <n-62-20-13>, in queue <gpuv100>, as user <tfehjo> in cluster <dcc> at Wed Nov 24 18:29:53 2021
</zhome/2b/7/117471> was used as the home directory.
</zhome/2b/7/117471/Thesis/train_scripts> was used as the working directory.
Started at Wed Nov 24 18:29:53 2021
Terminated at Wed Nov 24 19:01:07 2021
Results reported at Wed Nov 24 19:01:07 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
#BSUB -J METALEARN #The name the job will get
#BSUB -q gpuv100 #The queue the job will be committed to, here the GPU enabled queue
#BSUB -gpu "num=1:mode=exclusive_process" #How the job will be run on the VM, here I request 1 GPU with exclusive access i.e. only my c #BSUB -n 1 How many CPU cores my job request
#BSUB -W 04:00 #The maximum runtime my job have note that the queuing might enable shorter jobs earlier due to scheduling.
#BSUB -R "span[hosts=1]" #How many nodes the job requests
#BSUB -R "rusage[mem=12GB]" #How much RAM the job should have access to
#BSUB -R "select[gpu32gb]" #For requesting the extra big GPU w. 32GB of VRAM
#BSUB -o logs/OUTPUT.%J #Log file
#BSUB -e logs/ERROR.%J #Error log file
echo "Starting:"

cd ~/Thesis/metalearning
#cd /Users/theisferre/Documents/SPECIALE/Thesis/src/models

source ~/Thesis/venv-thesis/bin/activate

DATA_DIR=/zhome/2b/7/117471/Thesis/data/processed/aglation-non_augmented
TRAIN_SIZE=0.9
BATCH_TASK_SIZE=10
K_SHOT=5
ADAPTATION_STEPS=10
EPOCHS=150
ADAPT_LR=0.05
META_LR=0.001
EXCLUDE=citibike-tripdata-GRID,TLC2018-FHV-aug-REGION
LOG_DIR=/zhome/2b/7/117471/Thesis/ablation-study/non-augmented
HIDDEN_SIZE=46
DROPOUT_P=0.2
NODE_OUT_FEATURES=10

# citibike-tripdata-GRID,TLC2018-FHV-aug-REGION,citibike2014-tripdata-GRID,GM,yellow-taxi2020-nov-REGION,green,UBER2015-jan-june-GRID,LYFT,TLC2018-FHV-GRID,citibike2014-tripdata-REGION,UBER2015-jan-june-REGION,yellow-taxi2020-nov-GRID

python /zhome/2b/7/117471/Thesis/src/models/train_meta.py --data_dir $DATA_DIR --train_size $TRAIN_SIZE --batch_task_size $BATCH_TASK_SIZE \
--k_shot $K_SHOT --adaptation_steps $ADAPTATION_STEPS --epochs $EPOCHS --adapt_lr $ADAPT_LR --meta_lr $META_LR --log_dir $LOG_DIR \
--hidden_size $HIDDEN_SIZE --dropout_p $DROPOUT_P --node_out_features $NODE_OUT_FEATURES --exclude $EXCLUDE --gpu


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1842.42 sec.
    Max Memory :                                 3285 MB
    Average Memory :                             3098.88 MB
    Total Requested Memory :                     12288.00 MB
    Delta Memory :                               9003.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   1874 sec.
    Turnaround time :                            12571 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/ERROR.11148317> for stderr output of this job.

